{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "49a50587-5eaf-41b6-a177-281a652dacd3",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1e9d3d57-0742-4cbd-837a-4a68b0932e32",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\"\"\"Module containing convinience wrapped evaluation metrics.\"\"\"\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.metrics import average_precision_score, roc_auc_score,\\\n",
    "    accuracy_score, mean_absolute_error, mean_squared_error, balanced_accuracy_score\n",
    "\n",
    "\n",
    "def mean_absolute_percentage_error(y_true, y_pred):\n",
    "    return np.mean(np.abs((y_true - y_pred) / (y_true + 0.1))) * 100\n",
    "\n",
    "\n",
    "def to_one_hot(a, num_classes):\n",
    "    return np.squeeze(np.eye(num_classes)[a.reshape(-1)])\n",
    "\n",
    "\n",
    "def mc_metric_wrapper(metric, **kwargs):\n",
    "    \"\"\"Wrap metric for multi class classification.\n",
    "\n",
    "    If classifiction task is binary, select minority label as positive.\n",
    "    Otherwise compute weighted average over classes.\n",
    "    \"\"\"\n",
    "    def wrapped(y_true, y_score):\n",
    "        if y_true.ndim == 1 and y_score.ndim == 2:\n",
    "            # Multi class classification task where gt is given as int class\n",
    "            # indicator. First need to convert to one hot label.\n",
    "            n_classes = y_score.shape[-1]\n",
    "            y_true = to_one_hot(y_true, n_classes)\n",
    "        return metric(y_true, y_score, **kwargs)\n",
    "    return wrapped\n",
    "\n",
    "\n",
    "def accuracy(y_true, y_score):\n",
    "    \"\"\"Compute accuracy using one-hot representaitons.\"\"\"\n",
    "    if isinstance(y_true, list) and isinstance(y_score, list):\n",
    "        # Online scenario\n",
    "        if y_true[0].ndim == 2 and y_score[0].ndim == 2:\n",
    "            # Flatten to single (very long prediction)\n",
    "            y_true = np.concatenate(y_true, axis=0)\n",
    "            y_score = np.concatenate(y_score, axis=0)\n",
    "    if y_score.ndim == 3 and y_score.shape[-1] == 1:\n",
    "        y_score = np.ravel(y_score)\n",
    "        y_true = np.ravel(y_true).astype(int)\n",
    "        y_score = np.around(y_score).astype(int)\n",
    "    if y_true.ndim == 2 and y_true.shape[-1] != 1:\n",
    "        y_true = np.argmax(y_true, axis=-1)\n",
    "    if y_true.ndim == 2 and y_true.shape[-1] == 1:\n",
    "        y_true = np.round(y_true).astype(int)\n",
    "    if y_score.ndim == 2 and y_score.shape[-1] != 1:\n",
    "        y_score = np.argmax(y_score, axis=-1)\n",
    "    if y_score.ndim == 2 and y_score.shape[-1] == 1:\n",
    "        y_score = np.round(y_score).astype(int)\n",
    "    return accuracy_score(y_true, y_score)\n",
    "\n",
    "def balanced_accuracy(y_true, y_score):\n",
    "    \"\"\"Compute accuracy using one-hot representaitons.\"\"\"\n",
    "    if isinstance(y_true, list) and isinstance(y_score, list):\n",
    "        # Online scenario\n",
    "        if y_true[0].ndim == 2 and y_score[0].ndim == 2:\n",
    "            # Flatten to single (very long prediction)\n",
    "            y_true = np.concatenate(y_true, axis=0)\n",
    "            y_score = np.concatenate(y_score, axis=0)\n",
    "    if y_score.ndim == 3 and y_score.shape[-1] == 1:\n",
    "        y_score = np.ravel(y_score)\n",
    "        y_true = np.ravel(y_true).astype(int)\n",
    "        y_score = np.around(y_score).astype(int)\n",
    "    if y_true.ndim == 2 and y_true.shape[-1] != 1:\n",
    "        y_true = np.argmax(y_true, axis=-1)\n",
    "    if y_true.ndim == 2 and y_true.shape[-1] == 1:\n",
    "        y_true = np.round(y_true).astype(int)\n",
    "    if y_score.ndim == 2 and y_score.shape[-1] != 1:\n",
    "        y_score = np.argmax(y_score, axis=-1)\n",
    "    if y_score.ndim == 2 and y_score.shape[-1] == 1:\n",
    "        y_score = np.round(y_score).astype(int)\n",
    "    return balanced_accuracy_score(y_true, y_score)\n",
    "\n",
    "\n",
    "def mgp_wrapper(fn):\n",
    "    def wrapped(y_true, y_score):\n",
    "        if isinstance(y_true, list) and isinstance(y_score, list):\n",
    "            # Online scenario\n",
    "            if y_true[0].ndim == 2:\n",
    "                # Flatten to single (very long prediction)\n",
    "                y_true = np.concatenate(y_true, axis=0)\n",
    "                y_score = np.concatenate(y_score, axis=0)\n",
    "\n",
    "        assert y_true.size == y_score.size\n",
    "        return fn(np.ravel(y_true), np.ravel(y_score))\n",
    "    return wrapped\n",
    "\n",
    "\n",
    "def compute_prediction_utility(labels, predictions, dt_early=-12,\n",
    "                               dt_optimal=-6, dt_late=3.0, max_u_tp=1,\n",
    "                               min_u_fn=-2, u_fp=-0.05, u_tn=0,\n",
    "                               check_errors=True):\n",
    "    \"\"\"Compute utility score of physionet 2019 challenge.\"\"\"\n",
    "    # Check inputs for errors.\n",
    "    if check_errors:\n",
    "        if len(predictions) != len(labels):\n",
    "            raise Exception('Numbers of predictions and labels must be the same.')\n",
    "\n",
    "        for label in labels:\n",
    "            if not label in (0, 1):\n",
    "                raise Exception('Labels must satisfy label == 0 or label == 1.')\n",
    "\n",
    "        for prediction in predictions:\n",
    "            if not prediction in (0, 1):\n",
    "                raise Exception('Predictions must satisfy prediction == 0 or prediction == 1.')\n",
    "\n",
    "        if dt_early >= dt_optimal:\n",
    "            raise Exception('The earliest beneficial time for predictions must be before the optimal time.')\n",
    "\n",
    "        if dt_optimal >= dt_late:\n",
    "            raise Exception('The optimal time for predictions must be before the latest beneficial time.')\n",
    "\n",
    "    # Does the patient eventually have sepsis?\n",
    "    if np.any(labels):\n",
    "        is_septic = True\n",
    "        t_sepsis = np.argmax(labels) - dt_optimal\n",
    "    else:\n",
    "        is_septic = False\n",
    "        t_sepsis = float('inf')\n",
    "\n",
    "    n = len(labels)\n",
    "\n",
    "    # Define slopes and intercept points for utility functions of the form\n",
    "    # u = m * t + b.\n",
    "    m_1 = float(max_u_tp) / float(dt_optimal - dt_early)\n",
    "    b_1 = -m_1 * dt_early\n",
    "    m_2 = float(-max_u_tp) / float(dt_late - dt_optimal)\n",
    "    b_2 = -m_2 * dt_late\n",
    "    m_3 = float(min_u_fn) / float(dt_late - dt_optimal)\n",
    "    b_3 = -m_3 * dt_optimal\n",
    "\n",
    "    # Compare predicted and true conditions.\n",
    "    u = np.zeros(n)\n",
    "    for t in range(n):\n",
    "        if t <= t_sepsis + dt_late:\n",
    "            # TP\n",
    "            if is_septic and predictions[t]:\n",
    "                if t <= t_sepsis + dt_optimal:\n",
    "                    u[t] = max(m_1 * (t - t_sepsis) + b_1, u_fp)\n",
    "                elif t <= t_sepsis + dt_late:\n",
    "                    u[t] = m_2 * (t - t_sepsis) + b_2\n",
    "            # FP\n",
    "            elif not is_septic and predictions[t]:\n",
    "                u[t] = u_fp\n",
    "            # FN\n",
    "            elif is_septic and not predictions[t]:\n",
    "                if t <= t_sepsis + dt_optimal:\n",
    "                    u[t] = 0\n",
    "                elif t <= t_sepsis + dt_late:\n",
    "                    u[t] = m_3 * (t - t_sepsis) + b_3\n",
    "            # TN\n",
    "            elif not is_septic and not predictions[t]:\n",
    "                u[t] = u_tn\n",
    "\n",
    "    # Find total utility for patient.\n",
    "    return np.sum(u)\n",
    "\n",
    "\n",
    "def physionet2019_utility(y_true, y_score):\n",
    "    \"\"\"Compute physionet 2019 Sepsis eary detection utility.\n",
    "\n",
    "    Code based on:\n",
    "    \n",
    "\n",
    "    Args:\n",
    "        y_true:\n",
    "        y_score:\n",
    "\n",
    "    Returns:\n",
    "    \"\"\"\n",
    "    dt_early = -12\n",
    "    dt_optimal = -6\n",
    "    dt_late = 3.0\n",
    "\n",
    "    utilities = []\n",
    "    best_utilities = []\n",
    "    inaction_utilities = []\n",
    "\n",
    "    for labels, observed_predictions in zip(y_true, y_score):\n",
    "        observed_predictions = np.round(observed_predictions)\n",
    "        num_rows = len(labels)\n",
    "        best_predictions = np.zeros(num_rows)\n",
    "        inaction_predictions = np.zeros(num_rows)\n",
    "\n",
    "        if np.any(labels):\n",
    "            t_sepsis = np.argmax(labels) - dt_optimal\n",
    "            pred_begin = int(max(0, t_sepsis + dt_early))\n",
    "            pred_end = int(min(t_sepsis + dt_late + 1, num_rows))\n",
    "            best_predictions[pred_begin:pred_end] = 1\n",
    "\n",
    "        utilities.append(\n",
    "            compute_prediction_utility(labels, observed_predictions))\n",
    "        best_utilities.append(\n",
    "            compute_prediction_utility(labels, best_predictions))\n",
    "        inaction_utilities.append(\n",
    "            compute_prediction_utility(labels, inaction_predictions))\n",
    "\n",
    "    unnormalized_observed_utility = sum(utilities)\n",
    "    unnormalized_best_utility = sum(best_utilities)\n",
    "    unnormalized_inaction_utility = sum(inaction_utilities)\n",
    "    normalized_observed_utility = (\n",
    "        (unnormalized_observed_utility - unnormalized_inaction_utility)\n",
    "        / (unnormalized_best_utility - unnormalized_inaction_utility)\n",
    "    )\n",
    "    return normalized_observed_utility\n",
    "\n",
    "\n",
    "auroc = mgp_wrapper(roc_auc_score)\n",
    "auprc = mgp_wrapper(average_precision_score)\n",
    "auprc_micro = mc_metric_wrapper(average_precision_score, average='micro')\n",
    "auprc_macro = mc_metric_wrapper(average_precision_score, average='macro')\n",
    "auprc_weighted = mc_metric_wrapper(average_precision_score, average='weighted')\n",
    "\n",
    "auroc_micro = mc_metric_wrapper(roc_auc_score, average='micro')\n",
    "auroc_macro = mc_metric_wrapper(roc_auc_score, average='macro')\n",
    "auroc_weighted = mc_metric_wrapper(roc_auc_score, average='weighted')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e16ebd6b-3054-424e-9f62-e640ec968c06",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\"\"\"Definitions of possible tasks.\"\"\"\n",
    "import abc\n",
    "\n",
    "\n",
    "\n",
    "class Task(abc.ABC):\n",
    "    def __init__(self, class_weights=None):\n",
    "        self._class_weights = class_weights\n",
    "\n",
    "    @property\n",
    "    def class_weights(self):\n",
    "        return self._class_weights\n",
    "\n",
    "    @property\n",
    "    @abc.abstractmethod\n",
    "    def loss(self):\n",
    "        pass\n",
    "\n",
    "    @property\n",
    "    @abc.abstractmethod\n",
    "    def output_activation(self):\n",
    "        pass\n",
    "\n",
    "    @property\n",
    "    @abc.abstractmethod\n",
    "    def n_outputs(self):\n",
    "        pass\n",
    "\n",
    "    @property\n",
    "    @abc.abstractmethod\n",
    "    def metrics(self):\n",
    "        pass\n",
    "\n",
    "    @property\n",
    "    @abc.abstractmethod\n",
    "    def monitor_quantity(self):\n",
    "        pass\n",
    "\n",
    "    @property\n",
    "    @abc.abstractproperty\n",
    "    def direction_of_improvement(self):\n",
    "        pass\n",
    "\n",
    "\n",
    "class BinaryClassification(Task):\n",
    "    @property\n",
    "    def loss(self):\n",
    "        return 'binary_crossentropy'\n",
    "\n",
    "    @property\n",
    "    def output_activation(self):\n",
    "        return 'sigmoid'\n",
    "\n",
    "    @property\n",
    "    def n_outputs(self):\n",
    "        return 1\n",
    "\n",
    "    @property\n",
    "    def metrics(self):\n",
    "        # TODO: Extend by further metrics\n",
    "        return {\n",
    "            'auprc': metrics_module.auprc,\n",
    "            'auroc': metrics_module.auroc,\n",
    "            'accuracy': metrics_module.accuracy\n",
    "        }\n",
    "\n",
    "    @property\n",
    "    def monitor_quantity(self):\n",
    "        return 'auprc'\n",
    "\n",
    "    @property\n",
    "    def direction_of_improvement(self):\n",
    "        return 'max'\n",
    "\n",
    "\n",
    "class OnlineBinaryClassification(Task):\n",
    "    @property\n",
    "    def loss(self):\n",
    "        return 'binary_crossentropy'\n",
    "\n",
    "    @property\n",
    "    def output_activation(self):\n",
    "        return 'sigmoid'\n",
    "\n",
    "    @property\n",
    "    def n_outputs(self):\n",
    "        return (None, 1)\n",
    "\n",
    "    @property\n",
    "    def metrics(self):\n",
    "        return {\n",
    "            'auprc': metrics_module.auprc,\n",
    "            'auroc': metrics_module.auroc,\n",
    "            'accuracy': metrics_module.accuracy,\n",
    "            'balanced_accuracy': metrics_module.balanced_accuracy,\n",
    "            'physionet2019_utility': metrics_module.physionet2019_utility\n",
    "        }\n",
    "\n",
    "    @property\n",
    "    def monitor_quantity(self):\n",
    "        return 'balanced_accuracy'\n",
    "\n",
    "    @property\n",
    "    def direction_of_improvement(self):\n",
    "        return 'max'\n",
    "\n",
    "class MulticlassClassification(Task):\n",
    "    def __init__(self, n_classes, **kwargs):\n",
    "        self.n_classes = n_classes\n",
    "        super().__init__(**kwargs)\n",
    "\n",
    "    @property\n",
    "    def loss(self):\n",
    "        return 'categorical_crossentropy'\n",
    "\n",
    "    @property\n",
    "    def output_activation(self):\n",
    "        return 'softmax'\n",
    "\n",
    "    @property\n",
    "    def n_outputs(self):\n",
    "        return self.n_classes\n",
    "\n",
    "    @property\n",
    "    def metrics(self):\n",
    "        return {\n",
    "            'auprc_micro': metrics_module.auprc_micro,\n",
    "            'auprc_macro': metrics_module.auprc_macro,\n",
    "            'auprc_weighted': metrics_module.auprc_weighted,\n",
    "            'auroc_micro': metrics_module.auroc_micro,\n",
    "            'auroc_macro': metrics_module.auroc_macro,\n",
    "            'auroc_weighted': metrics_module.auroc_weighted,\n",
    "            'accuracy': metrics_module.accuracy\n",
    "        }\n",
    "\n",
    "    @property\n",
    "    def monitor_quantity(self):\n",
    "        return 'auprc_weighted'\n",
    "\n",
    "    @property\n",
    "    def direction_of_improvement(self):\n",
    "        return 'max'\n",
    "\n",
    "\n",
    "class MultilabelClassification(Task):\n",
    "    def __init__(self, n_classes, **kwargs):\n",
    "        self.n_classes = n_classes\n",
    "        super().__init__(**kwargs)\n",
    "\n",
    "    @property\n",
    "    def loss(self):\n",
    "        return 'binary_crossentropy'\n",
    "\n",
    "    @property\n",
    "    def output_activation(self):\n",
    "        return 'sigmoid'\n",
    "\n",
    "    @property\n",
    "    def n_outputs(self):\n",
    "        return self.n_classes\n",
    "\n",
    "    @property\n",
    "    def metrics(self):\n",
    "        return {\n",
    "            'auprc_micro': metrics_module.auprc_micro,\n",
    "            'auprc_macro': metrics_module.auprc_macro,\n",
    "            'auprc_weighted': metrics_module.auprc_weighted,\n",
    "            'auroc_micro': metrics_module.auroc_micro,\n",
    "            'auroc_macro': metrics_module.auroc_macro,\n",
    "            'auroc_weighted': metrics_module.auroc_weighted,\n",
    "        }\n",
    "\n",
    "    @property\n",
    "    def monitor_quantity(self):\n",
    "        return 'auprc_weighted'\n",
    "\n",
    "    @property\n",
    "    def direction_of_improvement(self):\n",
    "        return 'max'\n",
    "\n",
    "\n",
    "class Regression(Task):\n",
    "    def __init__(self, n_dimensions, is_positive):\n",
    "        self.n_dimensions = n_dimensions\n",
    "        self.is_positive = is_positive\n",
    "\n",
    "    @property\n",
    "    def loss(self):\n",
    "        return 'mean_squared_logarithmic_error'\n",
    "\n",
    "    @property\n",
    "    def output_activation(self):\n",
    "        return 'relu' if self.is_positive else 'linear'\n",
    "\n",
    "    @property\n",
    "    def n_outputs(self):\n",
    "        return self.n_dimensions\n",
    "\n",
    "    @property\n",
    "    def metrics(self):\n",
    "        return {\n",
    "            'mean_absolute_error': metrics_module.mean_absolute_error,\n",
    "            'mean_squared_error': metrics_module.mean_squared_error,\n",
    "            'mean_absolute_percentage_error':\n",
    "            metrics_module.mean_absolute_percentage_error\n",
    "        }\n",
    "\n",
    "    @property\n",
    "    def monitor_quantity(self):\n",
    "        return 'loss'\n",
    "\n",
    "    @property\n",
    "    def direction_of_improvement(self):\n",
    "        return 'min'\n",
    "\n",
    "\n",
    "DATASET_TO_TASK_MAPPING = {\n",
    "    'physionet2012': BinaryClassification(),\n",
    "    'physionet2019': OnlineBinaryClassification(\n",
    "        class_weights={0: 0.5553, 1: 5.0188}\n",
    "    ),\n",
    "    'mimic3_mortality': BinaryClassification(),\n",
    "    'mimic3_phenotyping': MultilabelClassification(25)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "04aedca5-351e-46df-82d8-acb175b62dfc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\"\"\"Implementation of GRU-D model.\n",
    "\n",
    "The below implementation is based on and adapted from\n",
    "https://github.com/PeterChe1990/GRU-D\n",
    "\n",
    "Which is published unter the MIT licence.\n",
    "\"\"\"\n",
    "from collections.abc import Sequence\n",
    "from collections import namedtuple\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras import activations, constraints, initializers, regularizers\n",
    "from tensorflow.python.keras.layers.recurrent import _generate_dropout_mask\n",
    "from tensorflow.python.keras.layers.recurrent import GRUCell\n",
    "from tensorflow.python.keras.utils.generic_utils import (\n",
    "    serialize_keras_object, custom_object_scope)\n",
    "\n",
    "\n",
    "GRUDInput = namedtuple('GRUDInput', ['values', 'mask', 'times'])\n",
    "GRUDState = namedtuple('GRUDState', ['h', 'x_keep', 's_prev'])\n",
    "\n",
    "__all__ = ['exp_relu', 'get_activation']\n",
    "\n",
    "_SUPPORTED_IMPUTATION = ['zero', 'forward', 'raw']\n",
    "\n",
    "\n",
    "def exp_relu(x):\n",
    "    return K.exp(-K.relu(x))\n",
    "\n",
    "\n",
    "def get_activation(identifier):\n",
    "    if identifier is None:\n",
    "        return None\n",
    "    with custom_object_scope({'exp_relu': exp_relu}):\n",
    "        return tf.keras.activations.get(identifier)\n",
    "\n",
    "\n",
    "class GRUDCell(GRUCell):\n",
    "    \"\"\"Cell class for the GRU-D layer. An extension of `GRUCell`.\n",
    "    Notice: Calling with only 1 tensor due to the limitation of Keras.\n",
    "    Building, computing the shape with the input_shape as a list of length 3.\n",
    "    # TODO: dynamic imputation\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, units, x_imputation='zero', input_decay='exp_relu',\n",
    "                 hidden_decay='exp_relu', use_decay_bias=True,\n",
    "                 feed_masking=True, masking_decay=None,\n",
    "                 decay_initializer='zeros', decay_regularizer=None,\n",
    "                 decay_constraint=None, **kwargs):\n",
    "        assert 'reset_after' not in kwargs or not kwargs['reset_after'], (\n",
    "            'Only the default GRU reset gate can be used in GRU-D.'\n",
    "        )\n",
    "        assert ('implementation' not in kwargs\n",
    "                or kwargs['implementation'] == 1), (\n",
    "                    'Only Implementation-1 (larger number of smaller operations) '\n",
    "                    'is supported in GRU-D.'\n",
    "                )\n",
    "\n",
    "        assert x_imputation in _SUPPORTED_IMPUTATION, (\n",
    "            'x_imputation {} argument is not supported.'.format(x_imputation)\n",
    "        )\n",
    "        self.x_imputation = x_imputation\n",
    "\n",
    "        self.input_decay = get_activation(input_decay)\n",
    "        self.hidden_decay = get_activation(hidden_decay)\n",
    "        self.use_decay_bias = use_decay_bias\n",
    "\n",
    "        assert (feed_masking or masking_decay is None\n",
    "                or masking_decay == 'None'), (\n",
    "                    'Mask needs to be fed into GRU-D to enable the mask_decay.'\n",
    "                )\n",
    "        self.feed_masking = feed_masking\n",
    "        if self.feed_masking:\n",
    "            self.masking_decay = get_activation(masking_decay)\n",
    "            self._masking_dropout_mask = None\n",
    "        else:\n",
    "            self.masking_decay = None\n",
    "\n",
    "        if (self.input_decay is not None\n",
    "            or self.hidden_decay is not None\n",
    "            or self.masking_decay is not None):\n",
    "            self.decay_initializer = initializers.get(decay_initializer)\n",
    "            self.decay_regularizer = regularizers.get(decay_regularizer)\n",
    "            self.decay_constraint = constraints.get(decay_constraint)\n",
    "\n",
    "        self._input_dim = None\n",
    "        # We need to wrap a try arround this as GRUCell sets state_size\n",
    "        try:\n",
    "            super().__init__(units, **kwargs)\n",
    "        except AttributeError:\n",
    "            pass\n",
    "\n",
    "    @property\n",
    "    def state_size(self):\n",
    "        return GRUDState(\n",
    "            h=self.units, x_keep=self._input_dim, s_prev=self._input_dim)\n",
    "\n",
    "    def get_initial_state(self, inputs, batch_size, dtype):\n",
    "        if inputs is None:\n",
    "            return GRUDState(\n",
    "                tf.zeros(tf.stack([batch_size, self.units])),\n",
    "                tf.zeros(tf.stack([batch_size, self._input_dim])),\n",
    "                tf.zeros(tf.stack([batch_size, self._input_dim]))\n",
    "            )\n",
    "        else:\n",
    "            if self.go_backwards:\n",
    "                return GRUDState(\n",
    "                    tf.zeros(tf.stack([batch_size, self.units])),\n",
    "                    tf.zeros(tf.stack([batch_size, self._input_dim])),\n",
    "                    tf.tile(\n",
    "                        tf.reduce_max(inputs.times, axis=1),\n",
    "                        [1, self._input_dim]\n",
    "                    )\n",
    "                )\n",
    "            else:\n",
    "                return GRUDState(\n",
    "                    tf.zeros(tf.stack([batch_size, self.units])),\n",
    "                    tf.zeros(tf.stack([batch_size, self._input_dim])),\n",
    "                    tf.tile(inputs.times[:, 0, :], [1, self._input_dim])\n",
    "                )\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            input_shape: A tuple of 3 shapes (from x, m, s, respectively)\n",
    "        \"\"\"\n",
    "        self._input_dim = input_shape.values[-1]\n",
    "        # Validate the shape of the input first. Borrow the idea from `_Merge`.\n",
    "        print(len(input_shape.times))\n",
    "        print(input_shape.times)\n",
    "        assert len(input_shape.times) == 2\n",
    "\n",
    "        # Borrow the logic from GRUCell for the same part.\n",
    "        super(GRUDCell, self).build(input_shape.values)\n",
    "\n",
    "        # Implementation of GRUCell changed, split the tensors here so we dont\n",
    "        # need to rewrite the code\n",
    "        self.kernel_z, self.kernel_r, self.kernel_h = tf.split(\n",
    "            self.kernel, 3, axis=-1)\n",
    "        (self.recurrent_kernel_z,\n",
    "         self.recurrent_kernel_r,\n",
    "         self.recurrent_kernel_h) = tf.split(self.recurrent_kernel, 3, axis=-1)\n",
    "        (self.input_bias_z,\n",
    "         self.input_bias_r,\n",
    "         self.input_bias_h) = tf.split(self.bias, 3, axis=-1)\n",
    "\n",
    "        # Build the own part of GRU-D.\n",
    "        if self.input_decay is not None:\n",
    "            self.input_decay_kernel = self.add_weight(\n",
    "                shape=(self._input_dim,),\n",
    "                name='input_decay_kernel',\n",
    "                initializer=self.decay_initializer,\n",
    "                regularizer=self.decay_regularizer,\n",
    "                constraint=self.decay_constraint\n",
    "            )\n",
    "            if self.use_decay_bias:\n",
    "                self.input_decay_bias = self.add_weight(\n",
    "                    shape=(self._input_dim,),\n",
    "                    name='input_decay_bias',\n",
    "                    initializer=self.bias_initializer,\n",
    "                    regularizer=self.bias_regularizer,\n",
    "                    constraint=self.bias_constraint\n",
    "                )\n",
    "        if self.hidden_decay is not None:\n",
    "            self.hidden_decay_kernel = self.add_weight(\n",
    "                shape=(self._input_dim, self.units),\n",
    "                name='hidden_decay_kernel',\n",
    "                initializer=self.decay_initializer,\n",
    "                regularizer=self.decay_regularizer,\n",
    "                constraint=self.decay_constraint\n",
    "            )\n",
    "            if self.use_decay_bias:\n",
    "                self.hidden_decay_bias = self.add_weight(\n",
    "                    shape=(self.units,),\n",
    "                    name='hidden_decay_bias',\n",
    "                    initializer=self.bias_initializer,\n",
    "                    regularizer=self.bias_regularizer,\n",
    "                    constraint=self.bias_constraint\n",
    "                )\n",
    "        if self.feed_masking:\n",
    "            self.masking_kernel = self.add_weight(\n",
    "                shape=(self._input_dim, self.units * 3),\n",
    "                name='masking_kernel',\n",
    "                initializer=self.kernel_initializer,\n",
    "                regularizer=self.kernel_regularizer,\n",
    "                constraint=self.kernel_constraint\n",
    "            )\n",
    "            if self.masking_decay is not None:\n",
    "                self.masking_decay_kernel = self.add_weight(\n",
    "                    shape=(self._input_dim,),\n",
    "                    name='masking_decay_kernel',\n",
    "                    initializer=self.decay_initializer,\n",
    "                    regularizer=self.decay_regularizer,\n",
    "                    constraint=self.decay_constraint\n",
    "                )\n",
    "                if self.use_decay_bias:\n",
    "                    self.masking_decay_bias = self.add_weight(\n",
    "                        shape=(self._input_dim,),\n",
    "                        name='masking_decay_bias',\n",
    "                        initializer=self.bias_initializer,\n",
    "                        regularizer=self.bias_regularizer,\n",
    "                        constraint=self.bias_constraint\n",
    "                    )\n",
    "            (\n",
    "                self.masking_kernel_z,\n",
    "                self.masking_kernel_r,\n",
    "                self.masking_kernel_h\n",
    "            ) = tf.split(self.masking_kernel, 3, axis=-1)\n",
    "        self.built = True\n",
    "\n",
    "    def reset_masking_dropout_mask(self):\n",
    "        self._masking_dropout_mask = None\n",
    "\n",
    "    def call(self, inputs, states, training=None):\n",
    "        \"\"\"We need to reimplmenet `call` entirely rather than reusing that\n",
    "        from `GRUCell` since there are lots of differences.\n",
    "        Args:\n",
    "            inputs: One tensor which is stacked by 3 inputs (x, m, s)\n",
    "                x and m are of shape (n_batch * input_dim).\n",
    "                s is of shape (n_batch, 1).\n",
    "            states: states and other values from the previous step.\n",
    "                (h_tm1, x_keep_tm1, s_prev_tm1)\n",
    "        \"\"\"\n",
    "        # Get inputs and states\n",
    "        input_x = inputs.values\n",
    "        input_m = inputs.mask\n",
    "        input_s = inputs.times\n",
    "\n",
    "        h_tm1, x_keep_tm1, s_prev_tm1 = states\n",
    "        # previous memory ([n_batch * self.units])\n",
    "        # previous input x ([n_batch * input_dim])\n",
    "        # and the subtraction term (of delta_t^d in Equation (2))\n",
    "        # ([n_batch * input_dim])\n",
    "        input_1m = 1. - tf.cast(input_m, tf.float32)\n",
    "        input_d = input_s - s_prev_tm1\n",
    "\n",
    "        dp_mask = self.get_dropout_mask_for_cell(\n",
    "                input_x, training, count=3) \n",
    "        rec_dp_mask = self.get_recurrent_dropout_mask_for_cell(\n",
    "                h_tm1, training, count=3)\n",
    "\n",
    "        if self.feed_masking:\n",
    "            if 0. < self.dropout < 1. and self._masking_dropout_mask is None:\n",
    "                self._masking_dropout_mask = _generate_dropout_mask(\n",
    "                    tf.ones_like(input_m, dtype=tf.float32),\n",
    "                    self.dropout,\n",
    "                    training=training,\n",
    "                    count=3)\n",
    "            m_dp_mask = self._masking_dropout_mask\n",
    "\n",
    "        # Compute decay if any\n",
    "        if self.input_decay is not None:\n",
    "            gamma_di = input_d * self.input_decay_kernel\n",
    "            if self.use_decay_bias:\n",
    "                gamma_di = K.bias_add(gamma_di, self.input_decay_bias)\n",
    "            gamma_di = self.input_decay(gamma_di)\n",
    "        if self.hidden_decay is not None:\n",
    "            gamma_dh = K.dot(input_d, self.hidden_decay_kernel)\n",
    "            if self.use_decay_bias:\n",
    "                gamma_dh = K.bias_add(gamma_dh, self.hidden_decay_bias)\n",
    "            gamma_dh = self.hidden_decay(gamma_dh)\n",
    "        if self.feed_masking and self.masking_decay is not None:\n",
    "            gamma_dm = input_d * self.masking_decay_kernel\n",
    "            if self.use_decay_bias:\n",
    "                gamma_dm = K.bias_add(gamma_dm, self.masking_decay_bias)\n",
    "            gamma_dm = self.masking_decay(gamma_dm)\n",
    "\n",
    "        # Get the imputed or decayed input if needed\n",
    "        # and `x_keep_t` for the next time step\n",
    "\n",
    "        if self.input_decay is not None:\n",
    "            x_keep_t = tf.where(input_m, input_x, x_keep_tm1)\n",
    "            x_t = tf.where(input_m, input_x, gamma_di * x_keep_t)\n",
    "        elif self.x_imputation == 'forward':\n",
    "            x_t = tf.where(input_m, input_x, x_keep_tm1)\n",
    "            x_keep_t = x_t\n",
    "        elif self.x_imputation == 'zero':\n",
    "            x_t = tf.where(input_m, input_x, K.zeros_like(input_x))\n",
    "            x_keep_t = x_t\n",
    "        elif self.x_imputation == 'raw':\n",
    "            x_t = input_x\n",
    "            x_keep_t = x_t\n",
    "        else:\n",
    "            raise ValueError('No input decay or invalid x_imputation '\n",
    "                             '{}.'.format(self.x_imputation))\n",
    "\n",
    "        # Get decayed hidden if needed\n",
    "        if self.hidden_decay is not None:\n",
    "            h_tm1d = gamma_dh * h_tm1\n",
    "        else:\n",
    "            h_tm1d = h_tm1\n",
    "\n",
    "        # Get decayed masking if needed\n",
    "        if self.feed_masking:\n",
    "            m_t = input_1m\n",
    "            if self.masking_decay is not None:\n",
    "                m_t = gamma_dm * m_t\n",
    "\n",
    "        # Apply the dropout\n",
    "        if 0. < self.dropout < 1.:\n",
    "            x_z, x_r, x_h = x_t * dp_mask[0], x_t * dp_mask[1], x_t * dp_mask[2]\n",
    "            if self.feed_masking:\n",
    "                m_z, m_r, m_h = (m_t * m_dp_mask[0],\n",
    "                                 m_t * m_dp_mask[1],\n",
    "                                 m_t * m_dp_mask[2]\n",
    "                                )\n",
    "        else:\n",
    "            x_z, x_r, x_h = x_t, x_t, x_t\n",
    "            if self.feed_masking:\n",
    "                m_z, m_r, m_h = m_t, m_t, m_t\n",
    "        if 0. < self.recurrent_dropout < 1.:\n",
    "            h_tm1_z, h_tm1_r = (h_tm1d * rec_dp_mask[0],\n",
    "                                         h_tm1d * rec_dp_mask[1],\n",
    "                                        )\n",
    "        else:\n",
    "            h_tm1_z, h_tm1_r = h_tm1d, h_tm1d\n",
    "\n",
    "        # Get z_t, r_t, hh_t\n",
    "        z_t = K.dot(x_z, self.kernel_z) + K.dot(h_tm1_z, self.recurrent_kernel_z)\n",
    "        r_t = K.dot(x_r, self.kernel_r) + K.dot(h_tm1_r, self.recurrent_kernel_r)\n",
    "        hh_t = K.dot(x_h, self.kernel_h)\n",
    "        if self.feed_masking:\n",
    "            z_t += K.dot(m_z, self.masking_kernel_z)\n",
    "            r_t += K.dot(m_r, self.masking_kernel_r)\n",
    "            hh_t += K.dot(m_h, self.masking_kernel_h)\n",
    "        if self.use_bias:\n",
    "            z_t = K.bias_add(z_t, self.input_bias_z)\n",
    "            r_t = K.bias_add(r_t, self.input_bias_r)\n",
    "            hh_t = K.bias_add(hh_t, self.input_bias_h)\n",
    "        z_t = self.recurrent_activation(z_t)\n",
    "        r_t = self.recurrent_activation(r_t)\n",
    "\n",
    "        if 0. < self.recurrent_dropout < 1.:\n",
    "            h_tm1_h = r_t * h_tm1d * rec_dp_mask[2]\n",
    "        else:\n",
    "            h_tm1_h = r_t * h_tm1d\n",
    "        hh_t = self.activation(hh_t + K.dot(h_tm1_h, self.recurrent_kernel_h))\n",
    "\n",
    "        # get h_t\n",
    "        h_t = z_t * h_tm1 + (1 - z_t) * hh_t\n",
    "\n",
    "        # get s_prev_t\n",
    "        s_prev_t = tf.where(input_m,\n",
    "                            K.tile(input_s, [1, self.state_size[-1]]),\n",
    "                            s_prev_tm1)\n",
    "        return h_t, GRUDState(h_t, x_keep_t, s_prev_t)\n",
    "\n",
    "    def get_config(self):\n",
    "        # Remember to record all args of the `__init__`\n",
    "        # which are not covered by `GRUCell`.\n",
    "        config = {'x_imputation': self.x_imputation,\n",
    "                  'input_decay': serialize_keras_object(self.input_decay),\n",
    "                  'hidden_decay': serialize_keras_object(self.hidden_decay),\n",
    "                  'use_decay_bias': self.use_decay_bias,\n",
    "                  'feed_masking': self.feed_masking,\n",
    "                  'masking_decay': serialize_keras_object(self.masking_decay),\n",
    "                  'decay_initializer': initializers.serialize(self.decay_initializer),\n",
    "                  'decay_regularizer': regularizers.serialize(self.decay_regularizer),\n",
    "                  'decay_constraint': constraints.serialize(self.decay_constraint)\n",
    "                 }\n",
    "        base_config = super(GRUDCell, self).get_config()\n",
    "        return dict(list(base_config.items()) + list(config.items()))\n",
    "\n",
    "\n",
    "class GRUD(tf.keras.layers.RNN):\n",
    "    def __init__(self, units, x_imputation='zero', input_decay='exp_relu',\n",
    "                 hidden_decay='exp_relu', use_decay_bias=True,\n",
    "                 feed_masking=True, masking_decay=None,\n",
    "                 decay_initializer='zeros', decay_regularizer=None,\n",
    "                 decay_constraint=None,  activation='tanh',\n",
    "                 recurrent_activation='hard_sigmoid',\n",
    "                 use_bias=True, kernel_initializer='glorot_uniform',\n",
    "                 recurrent_initializer='orthogonal', bias_initializer='zeros',\n",
    "                 kernel_regularizer=None, recurrent_regularizer=None,\n",
    "                 bias_regularizer=None, activity_regularizer=None,\n",
    "                 kernel_constraint=None, recurrent_constraint=None,\n",
    "                 bias_constraint=None, dropout=0., recurrent_dropout=0.,\n",
    "                 implementation=1, return_sequences=False, return_state=False,\n",
    "                 go_backwards=False, stateful=False, unroll=False,\n",
    "                 reset_after=False, **kwargs):\n",
    "        cell = GRUDCell(\n",
    "            units=units,\n",
    "            x_imputation=x_imputation,\n",
    "            input_decay=input_decay,\n",
    "            hidden_decay=hidden_decay,\n",
    "            use_decay_bias=use_decay_bias,\n",
    "            feed_masking=feed_masking,\n",
    "            masking_decay=masking_decay,\n",
    "            decay_initializer=decay_initializer,\n",
    "            decay_regularizer=decay_regularizer,\n",
    "            decay_constraint=decay_constraint,\n",
    "            activation=activation,\n",
    "            recurrent_activation=recurrent_activation,\n",
    "            use_bias=use_bias,\n",
    "            kernel_initializer=kernel_initializer,\n",
    "            recurrent_initializer=recurrent_initializer,\n",
    "            bias_initializer=bias_initializer,\n",
    "            kernel_regularizer=kernel_regularizer,\n",
    "            recurrent_regularizer=recurrent_regularizer,\n",
    "            bias_regularizer=bias_regularizer,\n",
    "            kernel_constraint=kernel_constraint,\n",
    "            recurrent_constraint=recurrent_constraint,\n",
    "            bias_constraint=bias_constraint,\n",
    "            dropout=dropout,\n",
    "            recurrent_dropout=recurrent_dropout,\n",
    "            implementation=implementation,\n",
    "            reset_after=reset_after,\n",
    "            dtype=kwargs.get('dtype')\n",
    "        )\n",
    "        super().__init__(\n",
    "            cell,\n",
    "            return_sequences=return_sequences,\n",
    "            return_state=return_state,\n",
    "            go_backwards=go_backwards,\n",
    "            stateful=stateful,\n",
    "            unroll=unroll,\n",
    "            **kwargs\n",
    "        )\n",
    "        self.activity_regularizer = regularizers.get(activity_regularizer)\n",
    "\n",
    "    def call(self, inputs, mask=None, training=None, initial_state=None):\n",
    "        self.cell.reset_dropout_mask()\n",
    "        self.cell.reset_recurrent_dropout_mask()\n",
    "        self.cell.reset_masking_dropout_mask()\n",
    "        return super().call(\n",
    "            inputs, mask=mask, training=training, initial_state=initial_state)\n",
    "\n",
    "    def get_config(self):\n",
    "        config = {\n",
    "            'units':\n",
    "                self.units,\n",
    "            'x_imputation': self.x_imputation,\n",
    "            'input_decay': serialize_keras_object(self.input_decay),\n",
    "            'hidden_decay': serialize_keras_object(self.hidden_decay),\n",
    "            'use_decay_bias': self.use_decay_bias,\n",
    "            'feed_masking': self.feed_masking,\n",
    "            'masking_decay': serialize_keras_object(self.masking_decay),\n",
    "            'decay_initializer': initializers.get(self.decay_initializer),\n",
    "            'decay_regularizer': regularizers.get(self.decay_regularizer),\n",
    "            'decay_constraint': constraints.get(self.decay_constraint),\n",
    "            'activation':\n",
    "                activations.serialize(self.activation),\n",
    "            'recurrent_activation':\n",
    "                activations.serialize(self.recurrent_activation),\n",
    "            'use_bias':\n",
    "                self.use_bias,\n",
    "            'kernel_initializer':\n",
    "                initializers.serialize(self.kernel_initializer),\n",
    "            'recurrent_initializer':\n",
    "                initializers.serialize(self.recurrent_initializer),\n",
    "            'bias_initializer':\n",
    "                initializers.serialize(self.bias_initializer),\n",
    "            'kernel_regularizer':\n",
    "                regularizers.serialize(self.kernel_regularizer),\n",
    "            'recurrent_regularizer':\n",
    "                regularizers.serialize(self.recurrent_regularizer),\n",
    "            'bias_regularizer':\n",
    "                regularizers.serialize(self.bias_regularizer),\n",
    "            'activity_regularizer':\n",
    "                regularizers.serialize(self.activity_regularizer),\n",
    "            'kernel_constraint':\n",
    "                constraints.serialize(self.kernel_constraint),\n",
    "            'recurrent_constraint':\n",
    "                constraints.serialize(self.recurrent_constraint),\n",
    "            'bias_constraint':\n",
    "                constraints.serialize(self.bias_constraint),\n",
    "            'dropout':\n",
    "                self.dropout,\n",
    "            'recurrent_dropout':\n",
    "                self.recurrent_dropout,\n",
    "            'implementation':\n",
    "                self.implementation,\n",
    "            'reset_after':\n",
    "                self.reset_after\n",
    "        }\n",
    "        base_config = super().get_config()\n",
    "        del base_config['cell']\n",
    "        return dict(list(base_config.items()) + list(config.items()))\n",
    "\n",
    "\n",
    "class GRUDModel(tf.keras.Model):\n",
    "    def __init__(self, output_activation, output_dims, n_units, dropout,\n",
    "                 recurrent_dropout):\n",
    "        self._config = {\n",
    "            name: val for name, val in locals().items()\n",
    "            if name not in ['self', '__class__']\n",
    "        }\n",
    "        super().__init__()\n",
    "        self.n_units = n_units\n",
    "        if isinstance(output_dims, Sequence):\n",
    "            # We have an online prediction scenario\n",
    "            assert output_dims[0] is None\n",
    "            self.return_sequences = True\n",
    "            output_dims = output_dims[1]\n",
    "        else:\n",
    "            self.return_sequences = False\n",
    "        self.rnn = GRUD(\n",
    "            n_units, dropout=dropout, recurrent_dropout=recurrent_dropout,\n",
    "            return_sequences=self.return_sequences\n",
    "        )\n",
    "        self.output_layer = tf.keras.layers.Dense(\n",
    "            output_dims, activation=output_activation)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        demo, times, values, measurements, lengths = input_shape\n",
    "        self.rnn.build(\n",
    "            GRUDInput(values=values, mask=measurements, times=times + (1,)))\n",
    "        self.demo_encoder = tf.keras.Sequential(\n",
    "            [\n",
    "                tf.keras.layers.Dense(self.n_units, activation='relu'),\n",
    "                tf.keras.layers.Dense(self.rnn.cell.state_size[0])\n",
    "            ],\n",
    "            name='demo_encoder'\n",
    "        )\n",
    "        self.demo_encoder.build(demo)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        demo, times, values, measurements, lengths = inputs\n",
    "        print(backend.is_keras_tensor(demo))\n",
    "        times = tf.expand_dims(times, -1)\n",
    "\n",
    "        demo_encoded = self.demo_encoder(demo)\n",
    "        initial_state = GRUDState(\n",
    "            demo_encoded,\n",
    "            tf.zeros(tf.stack([tf.shape(demo)[0], self.rnn.cell._input_dim])),\n",
    "            tf.tile(times[:, 0, :], [1, self.rnn.cell._input_dim])\n",
    "        )\n",
    "\n",
    "        mask = tf.sequence_mask(tf.squeeze(lengths, axis=-1), name='mask')\n",
    "        grud_output = self.rnn(\n",
    "            GRUDInput(\n",
    "                values=values,\n",
    "                mask=measurements,\n",
    "                times=times\n",
    "            ),\n",
    "            mask=mask,\n",
    "            initial_state=initial_state\n",
    "        )\n",
    "        return self.output_layer(grud_output)\n",
    "\n",
    "    def data_preprocessing_fn(self):\n",
    "        return None\n",
    "\n",
    "    @classmethod\n",
    "    def get_hyperparameters(cls):\n",
    "        from ..training_utils import HParamWithDefault\n",
    "        import tensorboard.plugins.hparams.api as hp\n",
    "        return [\n",
    "            HParamWithDefault(\n",
    "                'n_units',\n",
    "                hp.Discrete([32, 64, 128, 256, 512, 1024]),\n",
    "                default=32\n",
    "            ),\n",
    "            HParamWithDefault(\n",
    "                'dropout',\n",
    "                hp.Discrete([0.0, 0.1, 0.2, 0.3, 0.4]),\n",
    "                default=0.0\n",
    "            ),\n",
    "            HParamWithDefault(\n",
    "                'recurrent_dropout',\n",
    "                hp.Discrete([0.0, 0.1, 0.2, 0.3, 0.4]),\n",
    "                default=0.0\n",
    "            )\n",
    "        ]\n",
    "\n",
    "    @classmethod\n",
    "    def from_hyperparameter_dict(cls, task, hparams):\n",
    "        return cls(output_activation=task.output_activation,\n",
    "                   output_dims=task.n_outputs,\n",
    "                   n_units=hparams['n_units'],\n",
    "                   dropout=hparams['dropout'],\n",
    "                   recurrent_dropout=hparams['recurrent_dropout'])\n",
    "\n",
    "    @classmethod\n",
    "    def from_config(cls, config):\n",
    "        return cls(**config)\n",
    "\n",
    "    def get_config(self):\n",
    "        return self._config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3f228ded-212f-4437-82c9-c8bff0f6a3a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "task = BinaryClassification()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9d7c9d82-8241-4866-b397-7bba2e31ebb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparameter_dict = {\n",
    "    'n_units': 60,\n",
    "    'dropout': 0.2,\n",
    "    'recurrent_dropout': 0.2\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cddb881-043d-4b5f-8ed6-109a4086cd3f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4847584-f2e3-426e-8a10-1ffca3e055e8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "28aa4c19-1f3e-4475-8dc9-57334e1bf1a5",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "48b00a14-f39b-4757-8969-ee4dc0818def",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import gzip\n",
    "import pandas as pd\n",
    "import os\n",
    "def generate_data(data_path, output_dir, start_hour=0, end_hour=24):\n",
    "    data, oc, train_ind, valid_ind, test_ind = pickle.load(open(data_path, 'rb'))\n",
    "    # Filter labeled data in first 24h.\n",
    "    data = data.loc[data.ts_ind.isin(np.concatenate((train_ind, valid_ind, test_ind), axis=-1))]\n",
    "    data = data.loc[(data.hour>=start_hour)&(data.hour<=end_hour)]\n",
    "\n",
    "    oc = oc.loc[oc.ts_ind.isin(np.concatenate((train_ind, valid_ind, test_ind), axis=-1))]\n",
    "    # Fix age.\n",
    "    data.loc[(data.variable=='Age')&(data.value>200), 'value'] = 91.4\n",
    "    # Get y and N.\n",
    "    y = np.array(oc.sort_values(by='ts_ind')['in_hospital_mortality']).astype('float32')\n",
    "    N = data.ts_ind.max() + 1\n",
    "    # Get static data with mean fill and missingness indicator.\n",
    "    static_varis = ['Age', 'Gender']\n",
    "    ii = data.variable.isin(static_varis)\n",
    "    static_data = data.loc[ii]\n",
    "    data = data.loc[~ii]\n",
    "    def inv_list(l, start=0):\n",
    "        d = {}\n",
    "        for i in range(len(l)):\n",
    "            d[l[i]] = i+start\n",
    "        return d\n",
    "    static_var_to_ind = inv_list(static_varis)\n",
    "    D = len(static_varis)\n",
    "    demo = np.zeros((N, D))\n",
    "    for row in tqdm(static_data.itertuples()):\n",
    "        demo[row.ts_ind, static_var_to_ind[row.variable]] = row.value\n",
    "    # Normalize static data.\n",
    "    means = demo.mean(axis=0, keepdims=True)\n",
    "    stds = demo.std(axis=0, keepdims=True)\n",
    "    stds = (stds==0)*1 + (stds!=0)*stds\n",
    "    demo = (demo-means)/stds\n",
    "    # Trim to max len.\n",
    "    data = data.sample(frac=1)\n",
    "    print(data.groupby('ts_ind')['hour'].nunique().quantile([0.25, 0.5, 0.75, 0.9, 0.99]))\n",
    "\n",
    "    max_timestep = int(data.groupby('ts_ind')['hour'].nunique().quantile(0.99))\n",
    "\n",
    "    # Get N, V, var_to_ind.\n",
    "    N = data.ts_ind.max() + 1\n",
    "    varis = sorted(list(set(data.variable)))\n",
    "    V = len(varis)\n",
    "    def inv_list(l, start=0):\n",
    "        d = {}\n",
    "        for i in range(len(l)):\n",
    "            d[l[i]] = i+start\n",
    "        return d\n",
    "\n",
    "    var_to_ind = inv_list(varis, start=1)\n",
    "    data['vind'] = data.variable.map(var_to_ind)\n",
    "    data = data[['ts_ind', 'vind', 'hour', 'value']]\n",
    "    # Add obs index.\n",
    "    data = data.sort_values(by=['ts_ind', 'hour', 'vind']).reset_index(drop=True)\n",
    "    data = data.reset_index().rename(columns={'index':'obs_ind'})\n",
    "    data = data.merge(data.groupby('ts_ind').agg({'obs_ind':'min'}).reset_index().rename(columns={ \\\n",
    "                                                                'obs_ind':'first_obs_ind'}), on='ts_ind')\n",
    "    data['obs_ind'] = data['obs_ind'] - data['first_obs_ind']\n",
    "    # Find max_timestep.\n",
    "    print ('max_timestep', max_timestep)\n",
    "\n",
    "    times_inp = np.zeros((N, max_timestep), dtype='float32')\n",
    "    values_inp = np.zeros((N, max_timestep, V), dtype='float32')\n",
    "    mask_inp = np.zeros((N, max_timestep, V), dtype='int32')\n",
    "    lengths_inp = np.zeros(N, dtype='int32')\n",
    "\n",
    "    cur_time = None\n",
    "    time_index = 0\n",
    "    prev_ts_ind = 0\n",
    "    for row in tqdm(data.itertuples()):\n",
    "        # Check if to iterate to next patient\n",
    "        if time_index==max_timestep-1 and prev_ts_ind==row.ts_ind:\n",
    "            continue\n",
    "        # For first patient\n",
    "        if cur_time==None:\n",
    "            cur_time = row.hour\n",
    "            time_index = 0\n",
    "        # if different patient\n",
    "        elif prev_ts_ind!=row.ts_ind:\n",
    "            prev_ts_ind = row.ts_ind\n",
    "            lengths_inp[row.ts_ind] = time_index+1\n",
    "            time_index = 0\n",
    "            cur_time = row.hour\n",
    "        # If same patient but different time\n",
    "        elif cur_time!=row.hour:\n",
    "            time_index += 1\n",
    "            cur_time = row.hour\n",
    "        \n",
    "        v = row.vind-1 #variable\n",
    "        times_inp[row.ts_ind, time_index] = row.hour\n",
    "        values_inp[row.ts_ind, time_index, v] = row.value\n",
    "        mask_inp[row.ts_ind, time_index, v] = 1\n",
    "        \n",
    "    mask_inp = mask_inp.astype(np.bool)  \n",
    "    demo_inp = demo\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    for mode in ['train', 'val', 'test']:\n",
    "        if mode=='train':\n",
    "            ind = train_ind\n",
    "        elif mode=='valid':\n",
    "            ind = valid_ind\n",
    "        else:\n",
    "            ind = test_ind\n",
    "        demo = demo_inp[ind]\n",
    "        times = times_inp[ind]\n",
    "        values = values_inp[ind]\n",
    "        measurements = mask_inp[ind]\n",
    "        lengths = lengths_inp[ind]\n",
    "        label = y[ind]\n",
    "\n",
    "        with gzip.GzipFile(f'{output_dir}/{mode}_demo.npy.gz', 'w') as f:\n",
    "            np.save(f, demo)\n",
    "        with gzip.GzipFile(f'{output_dir}/{mode}_times.npy.gz', 'w') as f:\n",
    "            np.save(f, times) \n",
    "        with gzip.GzipFile(f'{output_dir}/{mode}_values.npy.gz', 'w') as f:\n",
    "            np.save(f, values)\n",
    "        with gzip.GzipFile(f'{output_dir}/{mode}_measurements.npy.gz', 'w') as f:\n",
    "            np.save(f, measurements) \n",
    "        with gzip.GzipFile(f'{output_dir}/{mode}_lengths.npy.gz', 'w') as f:\n",
    "            np.save(f, lengths)\n",
    "        with gzip.GzipFile(f'{output_dir}/{mode}_label.npy.gz', 'w') as f:\n",
    "            np.save(f, label) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a08ad507-9661-4e59-92a5-e1afa3e0bab1",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# generate_data(\n",
    "#     data_path='./mimic_iii_preprocessed.pkl', \n",
    "#     output_dir='data_grud', \n",
    "#     start_hour=0, \n",
    "#     end_hour=24\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f40142a3-c3fe-44ea-b536-e07443d8a4e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "mode = 'test'\n",
    "output_dir = './data_grud'\n",
    "with gzip.GzipFile(f'{output_dir}/{mode}_demo.npy.gz', 'r') as f:\n",
    "    demo = np.load(f)\n",
    "with gzip.GzipFile(f'{output_dir}/{mode}_times.npy.gz', 'r') as f:\n",
    "    times = np.load(f) \n",
    "with gzip.GzipFile(f'{output_dir}/{mode}_values.npy.gz', 'r') as f:\n",
    "    values = np.load(f)\n",
    "with gzip.GzipFile(f'{output_dir}/{mode}_measurements.npy.gz', 'r') as f:\n",
    "    measurements = np.load(f) \n",
    "with gzip.GzipFile(f'{output_dir}/{mode}_lengths.npy.gz', 'r') as f:\n",
    "    lengths = np.load(f)\n",
    "with gzip.GzipFile(f'{output_dir}/{mode}_label.npy.gz', 'r') as f:\n",
    "    label = np.load(f) \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d27a727d-e82a-45fd-a410-4bdb76650c65",
   "metadata": {},
   "outputs": [],
   "source": [
    "demo = tf.convert_to_tensor(demo)\n",
    "times = tf.convert_to_tensor(times)\n",
    "values = tf.convert_to_tensor(values)\n",
    "measurements = tf.convert_to_tensor(measurements)\n",
    "lengths = tf.convert_to_tensor(lengths)\n",
    "label = tf.convert_to_tensor(label)\n",
    "\n",
    "dataset = tf.data.Dataset.from_tensor_slices(((demo, times, values, measurements, lengths), label))\n",
    "\n",
    "n_samples = label.shape.as_list()[0]\n",
    "batch_size = 32\n",
    "\n",
    "import math\n",
    "\n",
    "steps_per_epoch = int(math.ceil(n_samples / batch_size))\n",
    "\n",
    "from tensorflow.data.experimental import AUTOTUNE\n",
    "\n",
    "prefetched_dataset = dataset.prefetch(AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdeee691-bf76-4a46-b57b-7884ac4ee922",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'strided_slice_1:0' shape=(2,) dtype=float32>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "demo_inp[0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5b7739dd-72b3-4f8a-b67f-84c8ab147e71",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.python.keras import backend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "23311e54-6ac6-4698-a372-37f8ee1085a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "backend.is_keras_tensor(demo_inp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "8b1140f4-1b0b-41f9-8c6a-3e521d155b81",
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "3d0628ab-63c4-4605-9e9e-faf9a371a2d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/FYP/szhong005/.local/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:64: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/FYP/szhong005/.local/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:497: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "2\n",
      "(None, 1)\n",
      "True\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "in converted code:\n\n    /tmp/ipykernel_13135/3752318483.py:522 call  *\n        demo_encoded = self.demo_encoder(demo)\n    /home/FYP/szhong005/.local/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/base_layer.py:854 __call__\n        outputs = call_fn(cast_inputs, *args, **kwargs)\n    /home/FYP/szhong005/.local/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/sequential.py:269 call\n        outputs = layer(inputs, **kwargs)\n    /home/FYP/szhong005/.local/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/base_layer.py:881 __call__\n        inputs, outputs, args, kwargs)\n    /home/FYP/szhong005/.local/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/base_layer.py:2043 _set_connectivity_metadata_\n        input_tensors=inputs, output_tensors=outputs, arguments=arguments)\n    /home/FYP/szhong005/.local/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/base_layer.py:2059 _add_inbound_node\n        input_tensors)\n    /home/FYP/szhong005/.local/lib/python3.7/site-packages/tensorflow_core/python/util/nest.py:536 map_structure\n        structure[0], [func(*x) for x in entries],\n    /home/FYP/szhong005/.local/lib/python3.7/site-packages/tensorflow_core/python/util/nest.py:536 <listcomp>\n        structure[0], [func(*x) for x in entries],\n    /home/FYP/szhong005/.local/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/base_layer.py:2058 <lambda>\n        inbound_layers = nest.map_structure(lambda t: t._keras_history.layer,\n\n    AttributeError: 'tuple' object has no attribute 'layer'\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_13135/4246669579.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mlengths_inp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mInput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGRUDModel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_hyperparameter_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhyperparameter_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdemo_inp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimes_inp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalues_inp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmeasurements_inp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlengths_inp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    852\u001b[0m                     \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbase_layer_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmark_as_return\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0macd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    853\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 854\u001b[0;31m                   \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcall_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcast_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    855\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    856\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOperatorNotAllowedInGraphError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/tensorflow_core/python/autograph/impl/api.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    235\u001b[0m       \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint:disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    236\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'ag_error_metadata'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 237\u001b[0;31m           \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mag_error_metadata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    238\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    239\u001b[0m           \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: in converted code:\n\n    /tmp/ipykernel_13135/3752318483.py:522 call  *\n        demo_encoded = self.demo_encoder(demo)\n    /home/FYP/szhong005/.local/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/base_layer.py:854 __call__\n        outputs = call_fn(cast_inputs, *args, **kwargs)\n    /home/FYP/szhong005/.local/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/sequential.py:269 call\n        outputs = layer(inputs, **kwargs)\n    /home/FYP/szhong005/.local/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/base_layer.py:881 __call__\n        inputs, outputs, args, kwargs)\n    /home/FYP/szhong005/.local/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/base_layer.py:2043 _set_connectivity_metadata_\n        input_tensors=inputs, output_tensors=outputs, arguments=arguments)\n    /home/FYP/szhong005/.local/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/base_layer.py:2059 _add_inbound_node\n        input_tensors)\n    /home/FYP/szhong005/.local/lib/python3.7/site-packages/tensorflow_core/python/util/nest.py:536 map_structure\n        structure[0], [func(*x) for x in entries],\n    /home/FYP/szhong005/.local/lib/python3.7/site-packages/tensorflow_core/python/util/nest.py:536 <listcomp>\n        structure[0], [func(*x) for x in entries],\n    /home/FYP/szhong005/.local/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/base_layer.py:2058 <lambda>\n        inbound_layers = nest.map_structure(lambda t: t._keras_history.layer,\n\n    AttributeError: 'tuple' object has no attribute 'layer'\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.layers import Input\n",
    "demo_inp = keras.Input(shape=(2,))\n",
    "times_inp = keras.Input(shape=(239,))\n",
    "values_inp = keras.Input(shape=(239,129))\n",
    "measurements_inp = keras.Input(shape=(239,129))\n",
    "lengths_inp = keras.Input(shape=(1,))\n",
    "model = GRUDModel.from_hyperparameter_dict(task, hyperparameter_dict)\n",
    "output = model((demo_inp, times_inp, values_inp, measurements_inp, lengths_inp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "42c33ab6-82d2-4679-a5bb-ea9fb5fa7388",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Tensors in list passed to 'values' of 'Pack' Op have types [int32, <NOT CONVERTIBLE TO TENSOR>] that don't all match.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/tensorflow_core/python/framework/op_def_library.py\u001b[0m in \u001b[0;36m_apply_op_helper\u001b[0;34m(self, op_type_name, name, **keywords)\u001b[0m\n\u001b[1;32m    471\u001b[0m                 \u001b[0mpreferred_dtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdefault_dtype\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 472\u001b[0;31m                 as_ref=input_arg.is_ref)\n\u001b[0m\u001b[1;32m    473\u001b[0m             if input_arg.number_attr and len(\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/tensorflow_core/python/framework/ops.py\u001b[0m in \u001b[0;36minternal_convert_n_to_tensor\u001b[0;34m(values, dtype, name, as_ref, preferred_dtype, ctx)\u001b[0m\n\u001b[1;32m   1361\u001b[0m             \u001b[0mpreferred_dtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpreferred_dtype\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1362\u001b[0;31m             ctx=ctx))\n\u001b[0m\u001b[1;32m   1363\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mret\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/tensorflow_core/python/framework/ops.py\u001b[0m in \u001b[0;36minternal_convert_to_tensor\u001b[0;34m(value, dtype, name, as_ref, preferred_dtype, ctx, accepted_result_types)\u001b[0m\n\u001b[1;32m   1296\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mret\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1297\u001b[0;31m       \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconversion_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mas_ref\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mas_ref\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1298\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/tensorflow_core/python/framework/constant_op.py\u001b[0m in \u001b[0;36m_constant_tensor_conversion_function\u001b[0;34m(v, dtype, name, as_ref)\u001b[0m\n\u001b[1;32m    285\u001b[0m   \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mas_ref\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 286\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mconstant\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    287\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/tensorflow_core/python/framework/constant_op.py\u001b[0m in \u001b[0;36mconstant\u001b[0;34m(value, dtype, shape, name)\u001b[0m\n\u001b[1;32m    226\u001b[0m   return _constant_impl(value, dtype, shape, name, verify_shape=False,\n\u001b[0;32m--> 227\u001b[0;31m                         allow_broadcast=True)\n\u001b[0m\u001b[1;32m    228\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/tensorflow_core/python/framework/constant_op.py\u001b[0m in \u001b[0;36m_constant_impl\u001b[0;34m(value, dtype, shape, name, verify_shape, allow_broadcast)\u001b[0m\n\u001b[1;32m    264\u001b[0m           \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverify_shape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverify_shape\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 265\u001b[0;31m           allow_broadcast=allow_broadcast))\n\u001b[0m\u001b[1;32m    266\u001b[0m   \u001b[0mdtype_value\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mattr_value_pb2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAttrValue\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtensor_value\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/tensorflow_core/python/framework/tensor_util.py\u001b[0m in \u001b[0;36mmake_tensor_proto\u001b[0;34m(values, dtype, shape, verify_shape, allow_broadcast)\u001b[0m\n\u001b[1;32m    436\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mvalues\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 437\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"None values not supported.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    438\u001b[0m     \u001b[0;31m# if dtype is provided, forces numpy array to be the type\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: None values not supported.",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_13135/1873995953.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     28\u001b[0m initial_state = GRUDState(\n\u001b[1;32m     29\u001b[0m     \u001b[0mdemo_encoded\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m     \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdemo_inp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcell\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_input_dim\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m     \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcell\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_input_dim\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m )\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/tensorflow_core/python/util/dispatch.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    178\u001b[0m     \u001b[0;34m\"\"\"Call target, and fall back on dispatchers if there is a TypeError.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    179\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 180\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    181\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mTypeError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    182\u001b[0m       \u001b[0;31m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/tensorflow_core/python/ops/array_ops.py\u001b[0m in \u001b[0;36mstack\u001b[0;34m(values, axis, name)\u001b[0m\n\u001b[1;32m   1152\u001b[0m                        (axis, -expanded_num_dims, expanded_num_dims))\n\u001b[1;32m   1153\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1154\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mgen_array_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1155\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1156\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/tensorflow_core/python/ops/gen_array_ops.py\u001b[0m in \u001b[0;36mpack\u001b[0;34m(values, axis, name)\u001b[0m\n\u001b[1;32m   6301\u001b[0m   \u001b[0maxis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_execute\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake_int\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"axis\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6302\u001b[0m   _, _, _op = _op_def_lib._apply_op_helper(\n\u001b[0;32m-> 6303\u001b[0;31m         \"Pack\", values=values, axis=axis, name=name)\n\u001b[0m\u001b[1;32m   6304\u001b[0m   \u001b[0m_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_op\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6305\u001b[0m   \u001b[0m_inputs_flat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_op\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/tensorflow_core/python/framework/op_def_library.py\u001b[0m in \u001b[0;36m_apply_op_helper\u001b[0;34m(self, op_type_name, name, **keywords)\u001b[0m\n\u001b[1;32m    498\u001b[0m                                 (prefix, dtype.name))\n\u001b[1;32m    499\u001b[0m               \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 500\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"%s that don't all match.\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mprefix\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    501\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    502\u001b[0m               raise TypeError(\n",
      "\u001b[0;31mTypeError\u001b[0m: Tensors in list passed to 'values' of 'Pack' Op have types [int32, <NOT CONVERTIBLE TO TENSOR>] that don't all match."
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.layers import Input\n",
    "\n",
    "n_units = 60\n",
    "dropout = 0.2\n",
    "recurrent_dropout = 0.2\n",
    "return_sequences = True\n",
    "output_dims = 1\n",
    "output_activation = 'sigmoid'\n",
    "\n",
    "demo_inp = Input(shape=(2))\n",
    "times_inp = Input(shape=(239))\n",
    "values_inp = Input(shape=(239,129))\n",
    "measurements_inp = Input(shape=(239,129))\n",
    "lengths_inp = Input(shape=(1))\n",
    "\n",
    "rnn = GRUD(\n",
    "    n_units, dropout=dropout, recurrent_dropout=recurrent_dropout,\n",
    "    return_sequences=return_sequences\n",
    ")\n",
    "demo_encoded = tf.keras.Sequential(\n",
    "    [\n",
    "        tf.keras.layers.Dense(n_units, activation='relu'),\n",
    "        tf.keras.layers.Dense(rnn.cell.state_size[0])\n",
    "    ],\n",
    "    name='demo_encoder'\n",
    ")(demo_inp)\n",
    "\n",
    "initial_state = GRUDState(\n",
    "    demo_encoded,\n",
    "    tf.zeros(tf.stack([tf.shape(demo_inp)[0], rnn.cell._input_dim])),\n",
    "    tf.tile(times[:, 0, :], [1, rnn.cell._input_dim])\n",
    ")\n",
    "\n",
    "mask = tf.sequence_mask(tf.squeeze(lengths, axis=-1), name='mask')\n",
    "\n",
    "grud_output = rnn(GRUDInput(values=values_inp, mask=measurements_inp, times=times_inp), mask=mask, initial_state=initial_state)\n",
    "\n",
    "\n",
    "\n",
    "output = tf.keras.layers.Dense(\n",
    "    output_dims, \n",
    "    activation=output_activation\n",
    ")(grud_output)\n",
    "\n",
    "\n",
    "model = Model([demo_inp, times_inp, values_inp, measurements_inp, length_inp], output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "95467a0b-509c-426b-a450-8e464ae31122",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GRUDModel.from_hyperparameter_dict(task, hyperparameter_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "6d00f4df-f5da-464e-b90b-93503fe23662",
   "metadata": {},
   "outputs": [],
   "source": [
    "optim = tf.keras.optimizers.Adam(learning_rate=0.0001)\n",
    "model.compile(\n",
    "    optimizer=optim,\n",
    "    loss='binary_crossentropy'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "3f77d842-81b9-4565-8d9b-77504c761445",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Expected a shuffled dataset but input dataset `x` is not shuffled. Please invoke `shuffle()` on input dataset.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-28 20:40:16.769645: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcuda.so.1\n",
      "2024-01-28 20:40:16.775693: E tensorflow/stream_executor/cuda/cuda_driver.cc:318] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\n",
      "2024-01-28 20:40:16.775723: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:169] retrieving CUDA diagnostic information for host: SCSEGPU-TC1-01\n",
      "2024-01-28 20:40:16.775731: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:176] hostname: SCSEGPU-TC1-01\n",
      "2024-01-28 20:40:16.775811: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:200] libcuda reported version is: 535.129.3\n",
      "2024-01-28 20:40:16.775837: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:204] kernel reported version is: 535.129.3\n",
      "2024-01-28 20:40:16.775843: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:310] kernel version seems to match DSO: 535.129.3\n",
      "2024-01-28 20:40:16.776155: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 AVX512F FMA\n",
      "2024-01-28 20:40:16.782233: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2600000000 Hz\n",
      "2024-01-28 20:40:16.782360: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x51f0b30 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
      "2024-01-28 20:40:16.782375: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "(None, 1)\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "int() argument must be a string, a bytes-like object or a number, not 'NoneType'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_69410/3823154989.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m ).history\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m         \u001b[0mmax_queue_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0mworkers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m         use_multiprocessing=use_multiprocessing)\n\u001b[0m\u001b[1;32m    728\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    729\u001b[0m   def evaluate(self,\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, model, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, **kwargs)\u001b[0m\n\u001b[1;32m    641\u001b[0m         \u001b[0msteps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    642\u001b[0m         \u001b[0mvalidation_split\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidation_split\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 643\u001b[0;31m         shuffle=shuffle)\n\u001b[0m\u001b[1;32m    644\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    645\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[0;34m(self, x, y, sample_weight, class_weight, batch_size, check_steps, steps_name, steps, validation_split, shuffle, extract_tensors_from_dataset)\u001b[0m\n\u001b[1;32m   2416\u001b[0m     \u001b[0;31m# First, we build the model on the fly if necessary.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2417\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2418\u001b[0;31m       \u001b[0mall_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_input\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict_inputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_build_model_with_inputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2419\u001b[0m       \u001b[0mis_build_called\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2420\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training.py\u001b[0m in \u001b[0;36m_build_model_with_inputs\u001b[0;34m(self, inputs, targets)\u001b[0m\n\u001b[1;32m   2619\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2620\u001b[0m       \u001b[0mcast_inputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2621\u001b[0;31m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_set_inputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcast_inputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2622\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mprocessed_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_dict_inputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2623\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training.py\u001b[0m in \u001b[0;36m_set_inputs\u001b[0;34m(self, inputs, outputs, training)\u001b[0m\n\u001b[1;32m   2706\u001b[0m           \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'training'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtraining\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2707\u001b[0m       \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2708\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2709\u001b[0m       \u001b[0;32mexcept\u001b[0m \u001b[0mNotImplementedError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2710\u001b[0m         \u001b[0;31m# This Model or a submodel is dynamic and hasn't overridden\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    822\u001b[0m           \u001b[0;31m# Build layer if applicable (if the `build` method has been\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    823\u001b[0m           \u001b[0;31m# overridden).\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 824\u001b[0;31m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_build\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    825\u001b[0m           \u001b[0mcast_inputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_cast_inputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    826\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m_maybe_build\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2144\u001b[0m         \u001b[0;31m# operations.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2145\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mtf_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmaybe_init_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2146\u001b[0;31m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_shapes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2147\u001b[0m       \u001b[0;31m# We must set self.built since user defined build functions are not\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2148\u001b[0m       \u001b[0;31m# constrained to set self.built.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_69410/3994447295.py\u001b[0m in \u001b[0;36mbuild\u001b[0;34m(self, input_shape)\u001b[0m\n\u001b[1;32m    505\u001b[0m         \u001b[0mdemo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmeasurements\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlengths\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput_shape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    506\u001b[0m         self.rnn.build(\n\u001b[0;32m--> 507\u001b[0;31m             GRUDInput(values=values, mask=measurements, times=times + (1,)))\n\u001b[0m\u001b[1;32m    508\u001b[0m         self.demo_encoder = tf.keras.Sequential(\n\u001b[1;32m    509\u001b[0m             [\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/tensorflow_core/python/keras/layers/recurrent.py\u001b[0m in \u001b[0;36mbuild\u001b[0;34m(self, input_shape)\u001b[0m\n\u001b[1;32m    542\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcell\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mLayer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    543\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcell\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuilt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 544\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcell\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep_input_shape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    545\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    546\u001b[0m     \u001b[0;31m# set or validate state_spec\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_69410/3994447295.py\u001b[0m in \u001b[0;36mbuild\u001b[0;34m(self, input_shape)\u001b[0m\n\u001b[1;32m    132\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m         \u001b[0;31m# Borrow the logic from GRUCell for the same part.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 134\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mGRUDCell\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_shape\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    135\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    136\u001b[0m         \u001b[0;31m# Implementation of GRUCell changed, split the tensors here so we dont\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/tensorflow_core/python/keras/utils/tf_utils.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(instance, input_shape)\u001b[0m\n\u001b[1;32m    304\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0minput_shape\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    305\u001b[0m       \u001b[0minput_shape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconvert_shapes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_shape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mto_tuples\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 306\u001b[0;31m     \u001b[0moutput_shape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minstance\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_shape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    307\u001b[0m     \u001b[0;31m# Return shapes from `fn` as TensorShapes.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    308\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0moutput_shape\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/tensorflow_core/python/keras/layers/recurrent.py\u001b[0m in \u001b[0;36mbuild\u001b[0;34m(self, input_shape)\u001b[0m\n\u001b[1;32m   1600\u001b[0m         \u001b[0minitializer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkernel_initializer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1601\u001b[0m         \u001b[0mregularizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkernel_regularizer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1602\u001b[0;31m         constraint=self.kernel_constraint)\n\u001b[0m\u001b[1;32m   1603\u001b[0m     self.recurrent_kernel = self.add_weight(\n\u001b[1;32m   1604\u001b[0m         \u001b[0mshape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munits\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/base_layer.py\u001b[0m in \u001b[0;36madd_weight\u001b[0;34m(self, name, shape, dtype, initializer, regularizer, trainable, constraint, partitioner, use_resource, synchronization, aggregation, **kwargs)\u001b[0m\n\u001b[1;32m    527\u001b[0m         \u001b[0mcollections\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcollections_arg\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    528\u001b[0m         \u001b[0msynchronization\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msynchronization\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 529\u001b[0;31m         aggregation=aggregation)\n\u001b[0m\u001b[1;32m    530\u001b[0m     \u001b[0mbackend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrack_variable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvariable\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    531\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/tensorflow_core/python/training/tracking/base.py\u001b[0m in \u001b[0;36m_add_variable_with_custom_getter\u001b[0;34m(self, name, shape, dtype, initializer, getter, overwrite, **kwargs_for_getter)\u001b[0m\n\u001b[1;32m    710\u001b[0m         \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    711\u001b[0m         \u001b[0minitializer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitializer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 712\u001b[0;31m         **kwargs_for_getter)\n\u001b[0m\u001b[1;32m    713\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    714\u001b[0m     \u001b[0;31m# If we set an initializer and the variable processed it, tracking will not\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/base_layer_utils.py\u001b[0m in \u001b[0;36mmake_variable\u001b[0;34m(name, shape, dtype, initializer, trainable, caching_device, validate_shape, constraint, use_resource, collections, synchronization, aggregation, partitioner)\u001b[0m\n\u001b[1;32m    137\u001b[0m       \u001b[0msynchronization\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msynchronization\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m       \u001b[0maggregation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maggregation\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 139\u001b[0;31m       shape=variable_shape if variable_shape else None)\n\u001b[0m\u001b[1;32m    140\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/tensorflow_core/python/ops/variables.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[1;32m    256\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    257\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcls\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mVariableV1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 258\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_variable_v1_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    259\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mcls\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mVariable\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    260\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_variable_v2_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/tensorflow_core/python/ops/variables.py\u001b[0m in \u001b[0;36m_variable_v1_call\u001b[0;34m(cls, initial_value, trainable, collections, validate_shape, caching_device, name, variable_def, dtype, expected_shape, import_scope, constraint, use_resource, synchronization, aggregation, shape)\u001b[0m\n\u001b[1;32m    217\u001b[0m         \u001b[0msynchronization\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msynchronization\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m         \u001b[0maggregation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maggregation\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 219\u001b[0;31m         shape=shape)\n\u001b[0m\u001b[1;32m    220\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    221\u001b[0m   def _variable_v2_call(cls,\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/tensorflow_core/python/ops/variables.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(**kwargs)\u001b[0m\n\u001b[1;32m    195\u001b[0m                         shape=None):\n\u001b[1;32m    196\u001b[0m     \u001b[0;34m\"\"\"Call on Variable class. Useful to force the signature.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 197\u001b[0;31m     \u001b[0mprevious_getter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mdefault_variable_creator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    198\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgetter\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_default_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_variable_creator_stack\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    199\u001b[0m       \u001b[0mprevious_getter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_make_getter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgetter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprevious_getter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/tensorflow_core/python/ops/variable_scope.py\u001b[0m in \u001b[0;36mdefault_variable_creator\u001b[0;34m(next_creator, **kwargs)\u001b[0m\n\u001b[1;32m   2501\u001b[0m         \u001b[0msynchronization\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msynchronization\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2502\u001b[0m         \u001b[0maggregation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maggregation\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2503\u001b[0;31m         shape=shape)\n\u001b[0m\u001b[1;32m   2504\u001b[0m   \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2505\u001b[0m     return variables.RefVariable(\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/tensorflow_core/python/ops/variables.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[1;32m    260\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_variable_v2_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    261\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 262\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mVariableMetaclass\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    263\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    264\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/tensorflow_core/python/ops/resource_variable_ops.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, initial_value, trainable, collections, validate_shape, caching_device, name, dtype, variable_def, import_scope, constraint, distribute_strategy, synchronization, aggregation, shape)\u001b[0m\n\u001b[1;32m   1404\u001b[0m           \u001b[0maggregation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maggregation\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1405\u001b[0m           \u001b[0mshape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1406\u001b[0;31m           distribute_strategy=distribute_strategy)\n\u001b[0m\u001b[1;32m   1407\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1408\u001b[0m   def _init_from_args(self,\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/tensorflow_core/python/ops/resource_variable_ops.py\u001b[0m in \u001b[0;36m_init_from_args\u001b[0;34m(self, initial_value, trainable, collections, caching_device, name, dtype, constraint, synchronization, aggregation, distribute_strategy, shape)\u001b[0m\n\u001b[1;32m   1535\u001b[0m           \u001b[0;32mwith\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Initializer\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice_context_manager\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1536\u001b[0m             initial_value = ops.convert_to_tensor(\n\u001b[0;32m-> 1537\u001b[0;31m                 \u001b[0minitial_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0minit_from_fn\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0minitial_value\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1538\u001b[0m                 name=\"initial_value\", dtype=dtype)\n\u001b[1;32m   1539\u001b[0m           \u001b[0;32mif\u001b[0m \u001b[0mshape\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/base_layer_utils.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    117\u001b[0m           (type(init_ops.Initializer), type(init_ops_v2.Initializer))):\n\u001b[1;32m    118\u001b[0m         \u001b[0minitializer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minitializer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 119\u001b[0;31m       \u001b[0minit_val\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0minitializer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    120\u001b[0m       \u001b[0mvariable_dtype\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbase_dtype\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0muse_resource\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/tensorflow_core/python/ops/init_ops.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, shape, dtype, partition_info)\u001b[0m\n\u001b[1;32m    513\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mpartition_info\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    514\u001b[0m       \u001b[0mscale_shape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpartition_info\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfull_shape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 515\u001b[0;31m     \u001b[0mfan_in\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfan_out\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_compute_fans\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscale_shape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    516\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"fan_in\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    517\u001b[0m       \u001b[0mscale\u001b[0m \u001b[0;34m/=\u001b[0m \u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1.\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfan_in\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/tensorflow_core/python/ops/init_ops.py\u001b[0m in \u001b[0;36m_compute_fans\u001b[0;34m(shape)\u001b[0m\n\u001b[1;32m   1437\u001b[0m     \u001b[0mfan_in\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mreceptive_field_size\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1438\u001b[0m     \u001b[0mfan_out\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mreceptive_field_size\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1439\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfan_in\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfan_out\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1440\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1441\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: int() argument must be a string, a bytes-like object or a number, not 'NoneType'"
     ]
    }
   ],
   "source": [
    "history = model.fit(\n",
    "    prefetched_dataset,\n",
    "    epochs=1,\n",
    "    steps_per_epoch=steps_per_epoch,\n",
    "    verbose=1\n",
    ").history\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dcbf232-f5ad-418f-bad0-b420a27760de",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:.conda-fyp_strats_37] *",
   "language": "python",
   "name": "conda-env-.conda-fyp_strats_37-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
