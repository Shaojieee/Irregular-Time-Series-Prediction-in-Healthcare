{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0709f656-8e6a-493f-a160-a21ab749feca",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras import backend as K\n",
    "from keras.engine.topology import Layer\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "from keras import activations\n",
    "\n",
    "\n",
    "class single_channel_interp(Layer):\n",
    "\n",
    "    def __init__(self, ref_points, hours_look_ahead, **kwargs):\n",
    "        self.ref_points = ref_points\n",
    "        self.hours_look_ahead = hours_look_ahead  # in hours\n",
    "        super(single_channel_interp, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        #input_shape [batch, features, time_stamp]\n",
    "        self.time_stamp = input_shape[2]\n",
    "        self.d_dim = input_shape[1] // 4\n",
    "        self.activation = activations.get('sigmoid')\n",
    "        self.kernel = self.add_weight(\n",
    "            name='kernel',\n",
    "            shape=(self.d_dim, ),\n",
    "            initializer=keras.initializers.Constant(value=0.0),\n",
    "            trainable=True)\n",
    "        super(single_channel_interp, self).build(input_shape)\n",
    "\n",
    "    def call(self, x, reconstruction=False):\n",
    "        self.reconstruction = reconstruction\n",
    "        x_t = x[:, :self.d_dim, :]\n",
    "        d = x[:, 2*self.d_dim:3*self.d_dim, :]\n",
    "        if reconstruction:\n",
    "            output_dim = self.time_stamp\n",
    "            m = x[:, 3*self.d_dim:, :]\n",
    "            ref_t = K.tile(d[:, :, None, :], (1, 1, output_dim, 1))\n",
    "        else:\n",
    "            m = x[:, self.d_dim: 2*self.d_dim, :]\n",
    "            ref_t = np.linspace(0, self.hours_look_ahead, self.ref_points)\n",
    "            output_dim = self.ref_points\n",
    "            ref_t.shape = (1, ref_t.shape[0])\n",
    "        #x_t = x_t*m\n",
    "        d = K.tile(d[:, :, :, None], (1, 1, 1, output_dim))\n",
    "        mask = K.tile(m[:, :, :, None], (1, 1, 1, output_dim))\n",
    "        x_t = K.tile(x_t[:, :, :, None], (1, 1, 1, output_dim))\n",
    "        norm = (d - ref_t)*(d - ref_t)\n",
    "        a = K.ones((self.d_dim, self.time_stamp, output_dim))\n",
    "        pos_kernel = K.log(1 + K.exp(self.kernel))\n",
    "        alpha = a*pos_kernel[:, np.newaxis, np.newaxis]\n",
    "        w = K.logsumexp(-alpha*norm + K.log(mask), axis=2)\n",
    "        w1 = K.tile(w[:, :, None, :], (1, 1, self.time_stamp, 1))\n",
    "        w1 = K.exp(-alpha*norm + K.log(mask) - w1)\n",
    "        y = K.sum(w1*x_t, axis=2)\n",
    "        if reconstruction:\n",
    "            rep1 = tf.concat([y, w], 1)\n",
    "        else:\n",
    "            w_t = K.logsumexp(-10.0*alpha*norm + K.log(mask),\n",
    "                              axis=2)  # kappa = 10\n",
    "            w_t = K.tile(w_t[:, :, None, :], (1, 1, self.time_stamp, 1))\n",
    "            w_t = K.exp(-10.0*alpha*norm + K.log(mask) - w_t)\n",
    "            y_trans = K.sum(w_t*x_t, axis=2)\n",
    "            rep1 = tf.concat([y, w, y_trans], 1)\n",
    "        return rep1\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        if self.reconstruction:\n",
    "            return (input_shape[0], 2*self.d_dim, self.time_stamp)\n",
    "        return (input_shape[0], 3*self.d_dim, self.ref_points)\n",
    "\n",
    "\n",
    "class cross_channel_interp(Layer):\n",
    "\n",
    "    def __init__(self, **kwargs):\n",
    "        super(cross_channel_interp, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.d_dim = input_shape[1] // 3\n",
    "        self.activation = activations.get('sigmoid')\n",
    "        self.cross_channel_interp = self.add_weight(\n",
    "            name='cross_channel_interp',\n",
    "            shape=(self.d_dim, self.d_dim),\n",
    "            initializer=keras.initializers.Identity(gain=1.0),\n",
    "            trainable=True)\n",
    "\n",
    "        super(cross_channel_interp, self).build(input_shape)\n",
    "\n",
    "    def call(self, x, reconstruction=False):\n",
    "        self.reconstruction = reconstruction\n",
    "        self.output_dim = K.int_shape(x)[-1]\n",
    "        cross_channel_interp = self.cross_channel_interp\n",
    "        y = x[:, :self.d_dim, :]\n",
    "        w = x[:, self.d_dim:2*self.d_dim, :]\n",
    "        intensity = K.exp(w)\n",
    "        y = tf.transpose(y, perm=[0, 2, 1])\n",
    "        w = tf.transpose(w, perm=[0, 2, 1])\n",
    "        w2 = w\n",
    "        w = K.tile(w[:, :, :, None], (1, 1, 1, self.d_dim))\n",
    "        den = K.logsumexp(w, axis=2)\n",
    "        w = K.exp(w2 - den)\n",
    "        mean = K.mean(y, axis=1)\n",
    "        mean = K.tile(mean[:, None, :], (1, self.output_dim, 1))\n",
    "        w2 = K.dot(w*(y - mean), cross_channel_interp) + mean\n",
    "        rep1 = tf.transpose(w2, perm=[0, 2, 1])\n",
    "        if reconstruction is False:\n",
    "            y_trans = x[:, 2*self.d_dim:3*self.d_dim, :]\n",
    "            y_trans = y_trans - rep1  # subtracting smooth from transient part\n",
    "            rep1 = tf.concat([rep1, intensity, y_trans], 1)\n",
    "        return rep1\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        if self.reconstruction:\n",
    "            return (input_shape[0], self.d_dim, self.output_dim)\n",
    "        return (input_shape[0], 3*self.d_dim, self.output_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4188afee-8cbd-4b0d-bc1c-10aef1285aeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Input, Dense, GRU, Lambda, Permute\n",
    "from keras.models import Model\n",
    "\n",
    "\n",
    "def interp_net(num_features, timestamp, ref_points, hours_look_ahead, units, recurrent_dropout):\n",
    "    main_input = Input(shape=(4*num_features, timestamp), name='input')\n",
    "    sci = single_channel_interp(ref_points, hours_look_ahead)\n",
    "    cci = cross_channel_interp()\n",
    "    interp = cci(sci(main_input))\n",
    "    reconst = cci(sci(main_input, reconstruction=True),\n",
    "                  reconstruction=True)\n",
    "    aux_output = Lambda(lambda x: x, name='aux_output')(reconst)\n",
    "    z = Permute((2, 1))(interp)\n",
    "    z = GRU(units, activation='tanh', recurrent_dropout=recurrent_dropout, dropout=recurrent_dropout)(z)\n",
    "    main_output = Dense(1, activation='sigmoid', name='main_output')(z)\n",
    "    model = Model([main_input], [main_output, aux_output])\n",
    "    \n",
    "    print(model.summary())\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "275911e6-0f50-4da1-bbe5-4a283b5394f6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "557deb73-9bde-40d4-88fc-681bf6b38ab0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "62f7cbd7-8c6e-4035-94d9-9c366b0d1447",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_customloss(feature_std, num_features):\n",
    "    def customloss(ytrue, ypred):\n",
    "        \"\"\" Autoencoder loss\n",
    "        \"\"\"\n",
    "        # standard deviation of each feature mentioned in paper for MIMIC_III data\n",
    "        wc = feature_std\n",
    "        wc.shape = (1, num_features)\n",
    "        y = ytrue[:, :num_features, :]\n",
    "        m2 = ytrue[:, 3*num_features:4*num_features, :]\n",
    "        m2 = 1 - m2\n",
    "        m1 = ytrue[:, num_features:2*num_features, :]\n",
    "        m = m1*m2\n",
    "        ypred = ypred[:, :num_features, :]\n",
    "        x = (y - ypred)*(y - ypred)\n",
    "        x = x*m\n",
    "        count = tf.reduce_sum(m, axis=2)\n",
    "        count = tf.where(count > 0, count, tf.ones_like(count))\n",
    "        x = tf.reduce_sum(x, axis=2)/count\n",
    "        x = x/(wc**2)  # dividing by standard deviation\n",
    "        x = tf.reduce_sum(x, axis=1)/num_features\n",
    "        return tf.reduce_mean(x)\n",
    "    return customloss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "ff533468-7ffe-43a0-a4fb-312bdd58b9a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from interp_net import load_data\n",
    "train_ip, train_op, valid_ip, valid_op, test_ip, test_op, feature_std = load_data(\n",
    "    data_dir='./data_interp_net'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "d6466592-754b-477e-a9a4-c192042e0691",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(516, 239)"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_ip.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1aedf404-34c9-4add-b939-1f357e3a67b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.predict(test_ip[0:8])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "63d4737c-d2c4-4093-b7e0-ffeb56d5bcc7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input (InputLayer)              (None, 516, 239)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "single_channel_interp_3 (single multiple             129         input[0][0]                      \n",
      "                                                                 input[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "cross_channel_interp_3 (cross_c multiple             16641       single_channel_interp_3[0][0]    \n",
      "                                                                 single_channel_interp_3[1][0]    \n",
      "__________________________________________________________________________________________________\n",
      "permute_3 (Permute)             (None, 96, 387)      0           cross_channel_interp_3[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "gru_3 (GRU)                     (None, 100)          146400      permute_3[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "main_output (Dense)             (None, 1)            101         gru_3[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "aux_output (Lambda)             (None, 129, 239)     0           cross_channel_interp_3[1][0]     \n",
      "==================================================================================================\n",
      "Total params: 163,271\n",
      "Trainable params: 163,271\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "timestamp = train_ip.shape[2]\n",
    "num_features = train_ip.shape[1] // 4\n",
    "\n",
    "model = interp_net(\n",
    "    num_features=num_features,\n",
    "    timestamp=timestamp,\n",
    "    ref_points=96,\n",
    "    hours_look_ahead=24,\n",
    "    units=100,\n",
    "    recurrent_dropout=0.2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "4a42054b-a5c9-4561-96a5-d0ba238a3b44",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "89624it [00:00, 692447.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.25     36.0\n",
      "0.50     56.0\n",
      "0.75     94.0\n",
      "0.90    143.0\n",
      "0.99    239.0\n",
      "Name: hour, dtype: float64\n",
      "max_timestep 239\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "17951791it [00:55, 322053.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vind\n",
      "1       149.247326\n",
      "2       858.097448\n",
      "3      1293.136791\n",
      "4         0.661482\n",
      "5        81.894825\n",
      "          ...     \n",
      "125       3.619471\n",
      "126      10.609918\n",
      "127      23.654603\n",
      "128       0.083971\n",
      "129       0.889520\n",
      "Name: value, Length: 129, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "from interp_net import generate_data\n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "data_path = './interp_net_mimic_iii_preprocessed.pkl'\n",
    "start_hour=0\n",
    "end_hour=24\n",
    "input_dropout=0.2\n",
    "\n",
    "\n",
    "data, oc, train_ind, valid_ind, test_ind = pickle.load(open(data_path, 'rb'))\n",
    "# Filter labeled data in first 24h.\n",
    "data = data.loc[data.ts_ind.isin(np.concatenate((train_ind, valid_ind, test_ind), axis=-1))]\n",
    "data = data.loc[(data.hour>=start_hour)&(data.hour<=end_hour)]\n",
    "\n",
    "oc = oc.loc[oc.ts_ind.isin(np.concatenate((train_ind, valid_ind, test_ind), axis=-1))]\n",
    "# Fix age.\n",
    "data.loc[(data.variable=='Age')&(data.value>200), 'value'] = 91.4\n",
    "# Get y and N.\n",
    "y = np.array(oc.sort_values(by='ts_ind')['in_hospital_mortality']).astype('float32')\n",
    "N = data.ts_ind.max() + 1\n",
    "# Get static data with mean fill and missingness indicator.\n",
    "static_varis = ['Age', 'Gender']\n",
    "ii = data.variable.isin(static_varis)\n",
    "static_data = data.loc[ii]\n",
    "data = data.loc[~ii]\n",
    "def inv_list(l, start=0):\n",
    "    d = {}\n",
    "    for i in range(len(l)):\n",
    "        d[l[i]] = i+start\n",
    "    return d\n",
    "static_var_to_ind = inv_list(static_varis)\n",
    "D = len(static_varis)\n",
    "demo = np.zeros((N, D))\n",
    "for row in tqdm(static_data.itertuples()):\n",
    "    demo[row.ts_ind, static_var_to_ind[row.variable]] = row.value\n",
    "# Normalize static data.\n",
    "means = demo.mean(axis=0, keepdims=True)\n",
    "stds = demo.std(axis=0, keepdims=True)\n",
    "stds = (stds==0)*1 + (stds!=0)*stds\n",
    "demo = (demo-means)/stds\n",
    "# Trim to max len.\n",
    "data = data.sample(frac=1)\n",
    "print(data.groupby('ts_ind')['hour'].nunique().quantile([0.25, 0.5, 0.75, 0.9, 0.99]))\n",
    "\n",
    "max_timestep = int(data.groupby('ts_ind')['hour'].nunique().quantile(0.99))\n",
    "\n",
    "# Get N, V, var_to_ind.\n",
    "N = data.ts_ind.max() + 1\n",
    "varis = sorted(list(set(data.variable)))\n",
    "V = len(varis)\n",
    "def inv_list(l, start=0):\n",
    "    d = {}\n",
    "    for i in range(len(l)):\n",
    "        d[l[i]] = i+start\n",
    "    return d\n",
    "\n",
    "var_to_ind = inv_list(varis, start=1)\n",
    "data['vind'] = data.variable.map(var_to_ind)\n",
    "data = data[['ts_ind', 'vind', 'hour', 'value']]\n",
    "# Add obs index.\n",
    "data = data.sort_values(by=['ts_ind', 'hour', 'vind']).reset_index(drop=True)\n",
    "data = data.reset_index().rename(columns={'index':'obs_ind'})\n",
    "data = data.merge(data.groupby('ts_ind').agg({'obs_ind':'min'}).reset_index().rename(columns={ \\\n",
    "                                                            'obs_ind':'first_obs_ind'}), on='ts_ind')\n",
    "data['obs_ind'] = data['obs_ind'] - data['first_obs_ind']\n",
    "# Find max_timestep.\n",
    "print ('max_timestep', max_timestep)\n",
    "\n",
    "\n",
    "print(data.groupby(by=['vind'])['value'].std().sort_index())\n",
    "feature_std = np.array(data.groupby(by=['vind'])['value'].std().sort_index())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "0a6e9f02-05ba-4934-9d32-e1dbd1375fcc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "vind\n",
       "1       149.247326\n",
       "2       858.097448\n",
       "3      1293.136791\n",
       "4         0.661482\n",
       "5        81.894825\n",
       "          ...     \n",
       "125       3.619471\n",
       "126      10.609918\n",
       "127      23.654603\n",
       "128       0.083971\n",
       "129       0.889520\n",
       "Name: value, Length: 129, dtype: float64"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.groupby(by=['vind'])['value'].std().sort_index()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:.conda-fyp_strats_37] *",
   "language": "python",
   "name": "conda-env-.conda-fyp_strats_37-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
