{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "80cdbcac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch\n",
    "import numpy as np\n",
    "from sklearn.metrics import roc_auc_score, precision_recall_curve, auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "59830cb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialise_linear_layer(layer):\n",
    "    if isinstance(layer, nn.Linear):\n",
    "        torch.nn.init.xavier_uniform_(layer.weight)\n",
    "        if layer.bias is not None:\n",
    "            torch.nn.init.zeros_(layer.bias)\n",
    "            \n",
    "def initialise_parameters(layer, method):\n",
    "    if isinstance(layer, nn.Parameter):\n",
    "        if method=='glorot_uniform':\n",
    "            torch.nn.init.xavier_uniform_(layer)\n",
    "        elif method=='zeros':\n",
    "            torch.nn.init.zeros_(layer)\n",
    "        elif method=='ones':\n",
    "            torch.nn.init.ones_(layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "e5b7da08",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CVE(nn.Module):\n",
    "    def __init__(self, hid_dim, output_dim):\n",
    "        super(CVE, self).__init__()\n",
    "        self.stack = nn.Sequential(\n",
    "            nn.Linear(in_features=1, out_features=hid_dim, bias=True),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(in_features=hid_dim, out_features=output_dim, bias=False)\n",
    "        )\n",
    "        self.stack.apply(initialise_linear_layer)\n",
    "        \n",
    "    def forward(self, X):\n",
    "        X = X.unsqueeze(dim=-1)\n",
    "        return self.stack(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "d7d0ad8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "    def __init__(self, d, hid_dim):\n",
    "        super(Attention, self).__init__()\n",
    "        self.stack = nn.Sequential(\n",
    "            nn.Linear(in_features=d, out_features=hid_dim, bias=True),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(in_features=hid_dim, out_features=1, bias=False)\n",
    "        )\n",
    "        self.softmax = nn.Softmax(dim=-2)\n",
    "        self.stack.apply(initialise_linear_layer)\n",
    "    \n",
    "    def forward(self, X, mask, mask_value=-1e30):\n",
    "        attn_weights = self.stack(X)\n",
    "        mask = torch.unsqueeze(mask, dim=-1)\n",
    "        attn_weights = mask*attn_weights + (1-mask)*mask_value\n",
    "        attn_weights = self.softmax(attn_weights)\n",
    "        \n",
    "        return attn_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "79f12ede",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    def __init__(self, d, N=2, h=8, dk=None, dv=None, dff=None, dropout=0, epsilon=1e-07):\n",
    "        super(Transformer, self).__init__()\n",
    "        \n",
    "        self.N, self.h, self.dk, self.dv, self.dff, self.dropout = N, h, dk, dv, dff, dropout\n",
    "        self.epsilon = epsilon * epsilon\n",
    "        if self.dk==None:\n",
    "            self.dk = d // self.h\n",
    "        if self.dv==None:\n",
    "            self.dv = d//self.h\n",
    "        if self.dff==None:\n",
    "            self.dff = 2*d\n",
    "        \n",
    "        self.Wq = nn.Parameter(torch.empty(self.N, self.h, d, self.dk))\n",
    "        initialise_parameters(self.Wq, 'glorot_uniform')\n",
    "        \n",
    "        self.Wk = nn.Parameter(torch.empty(self.N, self.h, d, self.dk))\n",
    "        initialise_parameters(self.Wk, 'glorot_uniform')\n",
    "        \n",
    "        self.Wv = nn.Parameter(torch.empty(self.N, self.h, d, self.dk))\n",
    "        initialise_parameters(self.Wv, 'glorot_uniform')\n",
    "        \n",
    "        self.Wo = nn.Parameter(torch.empty(self.N, self.dv*self.h, d))\n",
    "        initialise_parameters(self.Wo, 'glorot_uniform')\n",
    "        \n",
    "        \n",
    "        self.W1 = nn.Parameter(torch.empty(self.N, d, self.dff))\n",
    "        initialise_parameters(self.W1, 'glorot_uniform')\n",
    "        \n",
    "        self.b1 = nn.Parameter(torch.empty(self.N, self.dff))\n",
    "        initialise_parameters(self.b1, 'zeros')\n",
    "        \n",
    "        self.W2 = nn.Parameter(torch.empty(self.N, self.dff, d))\n",
    "        initialise_parameters(self.W2, 'glorot_uniform')\n",
    "        \n",
    "        self.b2 = nn.Parameter(torch.empty(self.N, d))\n",
    "        initialise_parameters(self.b2, 'zeros')\n",
    "        \n",
    "        \n",
    "        self.gamma = nn.Parameter(torch.empty(2*self.N,))\n",
    "        initialise_parameters(self.gamma, 'ones')\n",
    "        \n",
    "        self.beta = nn.Parameter(torch.empty(2*self.N,))\n",
    "        initialise_parameters(self.beta, 'zeros')\n",
    "        \n",
    "        \n",
    "        self.dropout_layer = nn.Dropout(p=self.dropout)\n",
    "        self.identity = nn.Identity()\n",
    "        \n",
    "    def forward(self, X, mask, mask_value=-1e-30):\n",
    "        mask = torch.unsqueeze(mask, dim=-2)\n",
    "        \n",
    "        for i in range(self.N):\n",
    "            mha_ops = []\n",
    "            for j in range(self.h):\n",
    "                q = torch.matmul(X, self.Wq[i,j,:,:])\n",
    "                k = torch.matmul(X, self.Wk[i,j,:,:]).permute(0,2,1)\n",
    "                v = torch.matmul(X, self.Wv[i,j,:,:])\n",
    "                A = torch.bmm(q, k)\n",
    "                A = mask * A + (1-mask) * mask_value\n",
    "                \n",
    "                def dropped_A():\n",
    "                    dp_mask = (torch.rand_like(A)>=self.dropout).type(dtype=torch.float32)\n",
    "                    return A*dp_mask + (1-dp_mask)*mask_value\n",
    "                # Dropout\n",
    "                if self.training:\n",
    "                    A = dropped_A()\n",
    "                else:\n",
    "                    A = self.identity(A)\n",
    "                    \n",
    "                A = nn.functional.softmax(A, dim=-1)\n",
    "                \n",
    "                mha_ops.append(torch.bmm(A, v))\n",
    "            \n",
    "            conc = torch.cat(mha_ops, dim=-1)\n",
    "            proj = torch.matmul(conc, self.Wo[i,:,:])\n",
    "            # Dropout\n",
    "            if self.training:\n",
    "                proj = self.identity(self.dropout_layer(proj))\n",
    "            else:\n",
    "                proj = self.identity(proj)\n",
    "            \n",
    "            # Add\n",
    "            X = X + proj\n",
    "            # Layer Normalisation\n",
    "            mean = torch.mean(X, dim=-1, keepdim=True)\n",
    "            variance = torch.mean(torch.square(X - mean), axis=-1 ,keepdims=True)\n",
    "            std = torch.sqrt(variance + self.epsilon)\n",
    "            X  = (X-mean)/std\n",
    "            X = X * self.gamma[2*i] + self.beta[2*i]\n",
    "            \n",
    "            # FFN\n",
    "            ffn_op = torch.add(torch.matmul(nn.functional.relu(torch.add(torch.matmul(X, self.W1[i,:,:]), self.b1[i,:])), self.W2[i,:,:]),self.b2[i,:])\n",
    "            # FFN Dropout\n",
    "            if self.training:\n",
    "                ffn_op = self.dropout_layer(ffn_op)\n",
    "            else:\n",
    "                ffn_op = self.identity(ffn_op)\n",
    "            \n",
    "            # Add\n",
    "            X = X + ffn_op\n",
    "            # Layer Normalisation\n",
    "            mean = torch.mean(X, dim=-1, keepdim=True)\n",
    "            variance = torch.mean(torch.square(X - mean), axis=-1 ,keepdims=True)\n",
    "            std = torch.sqrt(variance + self.epsilon)\n",
    "            X = (X-mean)/std\n",
    "            X = X*self.gamma[2*i+1] + self.beta[2*i+1]\n",
    "        return X            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "60c6293e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class STraTS(nn.Module):\n",
    "    def __init__(self, D, V, d, N, he, dropout, forecast=False):\n",
    "        super(STraTS, self).__init__()\n",
    "        total_parameters = 0\n",
    "        cve_units = int(np.sqrt(d))\n",
    "        # Inputs: max_len * batch_size\n",
    "        self.varis_stack = nn.Embedding(V+1, d)\n",
    "        num_params = sum(p.numel() for p in self.varis_stack.parameters())\n",
    "        print(f'varis_stack: {num_params}')\n",
    "        total_parameters += num_params\n",
    "        \n",
    "        self.values_stack = CVE(\n",
    "            hid_dim=cve_units, \n",
    "            output_dim=d\n",
    "        )        \n",
    "        num_params = sum(p.numel() for p in self.values_stack.parameters())\n",
    "        print(f'values_stack: {num_params}')\n",
    "        total_parameters += num_params\n",
    "        \n",
    "        self.times_stack = CVE(\n",
    "            hid_dim=cve_units, \n",
    "            output_dim=d\n",
    "        )        \n",
    "        num_params = sum(p.numel() for p in self.times_stack.parameters())\n",
    "        print(f'times_stack: {num_params}')\n",
    "        total_parameters += num_params\n",
    "        \n",
    "        \n",
    "        # Transformer Output = batch_size * max_len * d\n",
    "        self.cont_stack = Transformer(\n",
    "            d=d, \n",
    "            N=N, \n",
    "            h=he, \n",
    "            dk=None, \n",
    "            dv=None, \n",
    "            dff=None, \n",
    "            dropout=dropout, \n",
    "            epsilon=1e-07\n",
    "        )\n",
    "        num_params = sum(p.numel() for p in self.cont_stack.parameters())\n",
    "        print(f'cont_stack: {num_params}')\n",
    "        total_parameters += num_params\n",
    "        \n",
    "        # Attention Output = batch_size * max_len * 1 \n",
    "        self.attn_stack = Attention(\n",
    "            d=d,\n",
    "            hid_dim=2*d\n",
    "        )\n",
    "        num_params = sum(p.numel() for p in self.attn_stack.parameters())\n",
    "        print(f'attn_stack: {num_params}')\n",
    "        total_parameters += num_params\n",
    "        \n",
    "        # Demographics Input : batch_size * D\n",
    "        # Demographics Output: batch_size * d\n",
    "        self.demo_stack = nn.Sequential(\n",
    "            nn.Linear(in_features=D, out_features=2*d),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(in_features=2*d, out_features=d),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "        num_params = sum(p.numel() for p in self.demo_stack.parameters())\n",
    "        print(f'demo_stack: {num_params}')\n",
    "        total_parameters += num_params\n",
    "        \n",
    "        # Output Layer Inputs: Attention Weight * Time Series Embedding + Demographic Encoding = batch_size * (+d)\n",
    "        if forecast:\n",
    "            self.output_stack = nn.Linear(in_features=d+d, out_features=V)\n",
    "        else:\n",
    "            self.output_stack = nn.Sequential(\n",
    "                nn.Linear(in_features=d+d, out_features=1),\n",
    "                nn.Sigmoid()\n",
    "            )\n",
    "        num_params = sum(p.numel() for p in self.output_stack.parameters())\n",
    "        print(f'output_stack: {num_params}')\n",
    "        total_parameters += num_params\n",
    "        \n",
    "        print(f'Total Parameters: {total_parameters}')\n",
    "    \n",
    "    def forward(self, demo, times, values, varis):\n",
    "        \n",
    "        demo_enc = self.demo_stack(demo)\n",
    "        varis_emb = self.varis_stack(varis)\n",
    "        values_emb = self.values_stack(values)\n",
    "        times_emb = self.times_stack(times)\n",
    "        print(f'varis_emb: {varis_emb.shape}')\n",
    "        print(f'values_emb: {values_emb.shape}')\n",
    "        print(f'times_emb: {times_emb.shape}')\n",
    "        \n",
    "        comb_emb = varis_emb + values_emb + times_emb\n",
    "        print(f'comb_emb: {comb_emb.shape}')\n",
    "        \n",
    "        mask = torch.clamp(varis, 0,1)\n",
    "        print(f'Mask: {mask.shape}')\n",
    "        \n",
    "        cont_emb = self.cont_stack(comb_emb, mask)\n",
    "        print(f'cont_emb: {cont_emb.shape}')\n",
    "        \n",
    "        # Calculating the weights for cont_emb\n",
    "        attn_weights = self.attn_stack(cont_emb, mask)\n",
    "        print(f'attn_weights: {attn_weights.shape}')\n",
    "        \n",
    "        # Getting the weighted avg from the embeddings\n",
    "        fused_emb = torch.sum(cont_emb * attn_weights, dim=-2)\n",
    "        print(f'fused_emb: {fused_emb.shape}')\n",
    "        \n",
    "        # Combining Time Series Embedding with Demographic Embeddings\n",
    "        conc = torch.cat([fused_emb, demo_enc], dim=-1)\n",
    "        print(f'conc: {conc.shape}')\n",
    "        \n",
    "        # Generating Output\n",
    "        output = self.output_stack(conc)\n",
    "        print(f'output: {output.shape}')\n",
    "        \n",
    "        return output\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "e8d5b807",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "varis_stack: 6500\n",
      "values_stack: 364\n",
      "times_stack: 364\n",
      "cont_stack: 39508\n",
      "attn_stack: 5200\n",
      "demo_stack: 5350\n",
      "output_stack: 13029\n",
      "Total Parameters: 70315\n"
     ]
    }
   ],
   "source": [
    "model = STraTS(D=2, V=129, d=50, N=2, he=4, dropout=0, forecast=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "c5e21793",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "STraTS(\n",
       "  (varis_stack): Embedding(130, 50)\n",
       "  (values_stack): CVE(\n",
       "    (stack): Sequential(\n",
       "      (0): Linear(in_features=1, out_features=7, bias=True)\n",
       "      (1): Tanh()\n",
       "      (2): Linear(in_features=7, out_features=50, bias=False)\n",
       "    )\n",
       "  )\n",
       "  (times_stack): CVE(\n",
       "    (stack): Sequential(\n",
       "      (0): Linear(in_features=1, out_features=7, bias=True)\n",
       "      (1): Tanh()\n",
       "      (2): Linear(in_features=7, out_features=50, bias=False)\n",
       "    )\n",
       "  )\n",
       "  (cont_stack): Transformer(\n",
       "    (dropout_layer): Dropout(p=0, inplace=False)\n",
       "    (identity): Identity()\n",
       "  )\n",
       "  (attn_stack): Attention(\n",
       "    (stack): Sequential(\n",
       "      (0): Linear(in_features=50, out_features=100, bias=True)\n",
       "      (1): Tanh()\n",
       "      (2): Linear(in_features=100, out_features=1, bias=False)\n",
       "    )\n",
       "    (softmax): Softmax(dim=-2)\n",
       "  )\n",
       "  (demo_stack): Sequential(\n",
       "    (0): Linear(in_features=2, out_features=100, bias=True)\n",
       "    (1): Tanh()\n",
       "    (2): Linear(in_features=100, out_features=50, bias=True)\n",
       "    (3): Tanh()\n",
       "  )\n",
       "  (output_stack): Linear(in_features=100, out_features=129, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "0fe169dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "varis = torch.randint(high=130, size=(2,880))\n",
    "values = torch.rand((2,880))\n",
    "times = torch.rand((2,880))\n",
    "demo = torch.rand((2,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "5258c45e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "varis_emb: torch.Size([2, 880, 50])\n",
      "values_emb: torch.Size([2, 880, 50])\n",
      "times_emb: torch.Size([2, 880, 50])\n",
      "comb_emb: torch.Size([2, 880, 50])\n",
      "Mask: torch.Size([2, 880])\n",
      "cont_emb: torch.Size([2, 880, 50])\n",
      "attn_weights: torch.Size([2, 880, 1])\n",
      "fused_emb: torch.Size([2, 50])\n",
      "conc: torch.Size([2, 100])\n",
      "output: torch.Size([2, 129])\n"
     ]
    }
   ],
   "source": [
    "y_pred = model(demo, times,values, varis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "34f96db4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def forecast_loss(y_true, y_pred):\n",
    "    return torch.sum(y_true[:,V:]*(y_true[:,:V]- y_pred)**2, dim=-1)\n",
    "\n",
    "\n",
    "def get_results(y_true, y_pred):\n",
    "    precision, recall, thresholds = precision_recall_curve(y_true, y_pred)\n",
    "    pr_auc = auc(recall, precision)\n",
    "    minrp = np.minimum(precision, recall).max()\n",
    "    roc_auc = roc_auc_score(y_true, y_pred)\n",
    "    return [roc_auc, pr_auc, minrp]\n",
    "\n",
    "\n",
    "def mortality_loss(y_true, y_pred):\n",
    "    return nn.BCELoss(y_true, y_pred, reduction='mean')\n",
    "\n",
    "\n",
    "class Evaluation_Callback():\n",
    "    def __init__(self, val_dataloader):\n",
    "        self.val_dataloader = val_dataloader\n",
    "        self.logs = {'epoch': [], 'pr_auc': [], 'roc_auc': [], 'min_rp': [], 'loss': []}\n",
    "\n",
    "    def on_epoch_end(self, model, epoch, loss_fn):\n",
    "        model.eval()\n",
    "        Y = []\n",
    "        Y_pred = []\n",
    "        loss\n",
    "        with torch.no_grad():\n",
    "            for X, y in self.val_dataloader:\n",
    "                y_pred = model.predict(X)\n",
    "                loss += loss_fn(y_pred, y).detach().cpu().item() * len(y)\n",
    "                Y.append(y); Y_pred.append(y_pred)\n",
    "                \n",
    "                \n",
    "        loss /= len(self.val_dataloader)\n",
    "        precision, recall, thresholds = precision_recall_curve(Y, Y_pred)\n",
    "        pr_auc = auc(recall, precision)\n",
    "        roc_auc = roc_auc_score(Y, Y_pred)\n",
    "        min_rp = np.minimum(precision, recall).max()\n",
    "        self.logs['epoch'].append(epoch) \n",
    "        self.logs['pr_auc'].append(pr_auc); self.logs['roc_auc'].append(roc_auc); self['min_rp'].append(min_rp)\n",
    "        self.logs['loss'].append(loss);\n",
    "        print(f'Val Metrics: PR_AUC: {pr_auc:.6f} ROC_AUC: {roc_auc:.6f} MIN_RP: {min_rp:.6f} BCE_LOSS: {loss:.6f}')\n",
    "    \n",
    "        return pr_auc, roc_auc, min_rp, loss\n",
    "                   \n",
    "    def get_logs():\n",
    "        return pd.DataFrame(self.logs)\n",
    "              \n",
    "\n",
    "class Early_Stopper():\n",
    "    def __init__(self, patience=5, min_delta=0, mode='max', restore_best_weights=False):\n",
    "        self.patience = patience\n",
    "        self.wait = 0\n",
    "        self.min_delta = min_delta\n",
    "        self.mode = mode\n",
    "        self.stopped_epoch = 0\n",
    "        self.restore_best_weights = restore_best_weights\n",
    "        self.best_weights = None\n",
    "        self.best = np.inf if mode=='min' else -np.inf\n",
    "        \n",
    "        if mode=='min':\n",
    "            self.monitor_op = np.less\n",
    "        elif mode=='max':\n",
    "            self.monitor_op = np.greater\n",
    "              \n",
    "    def on_epoch_end(self, model, loss, epoch):\n",
    "        if self.monitor_op(loss - self.min_delta, self.best):\n",
    "            self.best = loss\n",
    "            self.wait = 0\n",
    "            if self.restore_best_weights:\n",
    "                self.best_weights = model.state_dict()\n",
    "        else:\n",
    "            self.wait += 1\n",
    "            \n",
    "            if self.wait>=self.patience:\n",
    "                self.stopped_epoch = epoch\n",
    "                print(f'Early Stopping at Epoch {epoch} with best loss of {self.best:.6f}')\n",
    "                if self.restore_best_weights:\n",
    "                      print(f'Restoring best weights at Epoch {self.stopped_epoch-self.wait}')\n",
    "                      model.load_state_dict(self.best_weights)\n",
    "        return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "034827b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class MultipleInputsDataset(Dataset):\n",
    "    def __init__(self, X, Y):\n",
    "        super(MultipleInputsDataset, self).__init__()\n",
    "        \n",
    "        self.X = X\n",
    "        self.Y = Y\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        \n",
    "        return *[x[idx] for x in self.X], self.Y[idx]\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "ad35d2a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import gzip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "ba346e49",
   "metadata": {},
   "outputs": [],
   "source": [
    "V = 129\n",
    "def forecast_loss(y_true, y_pred, V):\n",
    "    return torch.sum(y_true[:,V:]*(y_true[:,:V]- y_pred)**2, dim=-1).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "d8ec5543",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 129])"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "d686ce95",
   "metadata": {},
   "outputs": [],
   "source": [
    "f = gzip.GzipFile('/home/FYP/szhong005/fyp/multi_modal/STraTS_torch/forecast_datasets/val_y.npy.gz', \"r\")\n",
    "val_y = np.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "c418a4c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true = torch.tensor(val_y[0:2],dtype=torch.float32, requires_grad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "68721c5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "forecast_loss(y_true, y_pred, V).backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "042e9a45",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-68-b5a1f2ab0484>:1: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  y_pred.grad\n"
     ]
    }
   ],
   "source": [
    "y_pred.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b073d37",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:.conda-fyp_multi_modal] *",
   "language": "python",
   "name": "conda-env-.conda-fyp_multi_modal-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
