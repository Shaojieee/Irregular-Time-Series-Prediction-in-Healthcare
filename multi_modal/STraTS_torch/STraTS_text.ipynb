{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "import os\n",
    "import gzip\n",
    "from tqdm import tqdm\n",
    "from data import load_mortality_dataset, pad_text_data, normalise_time\n",
    "from torch.utils.data import DataLoader\n",
    "from strats_text_model import load_Bert\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./mortality_mimic_3_benchmark/train_texts.pkl', 'rb') as f:\n",
    "    text = pickle.load(f)\n",
    "with open('./mortality_mimic_3_benchmark/train_text_times.pkl', 'rb') as f:\n",
    "    time = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14681"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14681"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at yikuan8/Clinical-Longformer were not used when initializing LongformerModel: ['lm_head.layer_norm.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.weight', 'lm_head.decoder.bias', 'lm_head.dense.bias', 'lm_head.bias', 'lm_head.decoder.weight']\n",
      "- This IS expected if you are initializing LongformerModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing LongformerModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of LongformerModel were not initialized from the model checkpoint at yikuan8/Clinical-Longformer and are newly initialized: ['longformer.pooler.dense.bias', 'longformer.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "_, _, tokenizer = load_Bert(\n",
    "    text_encoder_model = 'bioLongformer'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100it [00:00, 179.60it/s]\n",
      "100it [00:00, 208.71it/s]\n",
      "100it [00:00, 222.33it/s]\n"
     ]
    }
   ],
   "source": [
    "train, val, test, V, D = load_mortality_dataset(\n",
    "    data_dir='./mortality_mimic_3_benchmark', \n",
    "    with_text=True, \n",
    "    tokenizer=tokenizer, \n",
    "    text_padding=True, \n",
    "    text_max_len=1024, \n",
    "    text_model='bioLongformer', \n",
    "    period_length=48, \n",
    "    num_notes=3,\n",
    "    debug=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(train, batch_size=2, collate_fn=pad_text_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/shaojieee/Desktop/fyp/multi_modal/STraTS_torch/data.py:635: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  X_text_times = pad_sequence([torch.tensor(time, dtype=torch.float) for time in X_text_times],batch_first=True,padding_value=0)\n",
      "/Users/shaojieee/Desktop/fyp/multi_modal/STraTS_torch/data.py:636: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  X_text_time_mask = pad_sequence([torch.tensor(time_mask, dtype=torch.long) for time_mask in X_text_time_mask],batch_first=True,padding_value=0)\n"
     ]
    }
   ],
   "source": [
    "X_demos, X_times, X_values, X_varis, Y, X_text_tokens, X_text_attention_mask, X_text_times, X_text_time_mask, X_text_feature_varis = iter(train_dataloader).next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 500])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_times.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0667,  0.0667,  0.0667,  0.0667,  0.0667,  0.0667,  0.1500,  0.1500,\n",
       "          0.1500,  0.1500,  0.1500,  0.1500,  0.2333,  0.2333,  0.2333,  0.2333,\n",
       "          0.2333,  0.2333,  0.3167,  0.3167,  0.3167,  0.3167,  0.3167,  0.3167,\n",
       "          0.4000,  0.4000,  0.4000,  0.4000,  0.4000,  0.4000,  0.4833,  0.4833,\n",
       "          0.4833,  0.4833,  0.4833,  0.4833,  0.5167,  0.5167,  0.5167,  0.5167,\n",
       "          0.5167,  0.5167,  0.5667,  0.5667,  0.5667,  0.5667,  0.5667,  0.5667,\n",
       "          0.6500,  0.6500,  0.6500,  0.6500,  0.6500,  0.6500,  0.7333,  0.7333,\n",
       "          0.7333,  0.7333,  0.7333,  0.7333,  0.8167,  0.8167,  0.8167,  0.8167,\n",
       "          0.8167,  0.8167,  0.9000,  0.9000,  0.9000,  0.9000,  0.9000,  0.9000,\n",
       "          0.9833,  0.9833,  0.9833,  0.9833,  0.9833,  0.9833,  1.0667,  1.0667,\n",
       "          1.0667,  1.0667,  1.0667,  1.0667,  1.1500,  1.1500,  1.1500,  1.1500,\n",
       "          1.1500,  1.1500,  1.2333,  1.2333,  1.2333,  1.2333,  1.2333,  1.2333,\n",
       "          1.2333,  1.2333,  1.2333,  1.2333,  1.3167,  1.3167,  1.3167,  1.3167,\n",
       "          1.3167,  1.3167,  1.4000,  1.4000,  1.4000,  1.4000,  1.4000,  1.4000,\n",
       "          2.2333,  2.2333,  2.2333,  2.2333,  2.2333,  2.2333,  3.2333,  3.2333,\n",
       "          3.2333,  3.2333,  3.2333,  3.2333,  3.2333,  3.7333,  3.7333,  3.7333,\n",
       "          3.7333,  3.7333,  3.7333,  3.9833,  3.9833,  3.9833,  3.9833,  3.9833,\n",
       "          4.2333,  4.2333,  4.2333,  4.2333,  4.2333,  4.8167,  4.8167,  4.8167,\n",
       "          4.8167,  4.8167,  4.9000,  4.9000,  4.9000,  4.9000,  4.9000,  5.0667,\n",
       "          5.0667,  5.0667,  5.0667,  5.0667,  5.1500,  5.1500,  5.1500,  5.1500,\n",
       "          5.1500,  5.2333,  5.2333,  5.2333,  5.2333,  5.2333,  5.2333,  5.2333,\n",
       "          5.2333,  5.2333,  5.2333,  5.2333,  5.2667,  5.3667,  5.3667,  5.3667,\n",
       "          5.6000,  6.8333,  6.8333,  7.7167,  7.7167,  7.7167,  8.4000,  8.4000,\n",
       "          9.0833,  9.0833,  9.9500,  9.9500, 10.4833, 10.7000, 11.8167, 11.8167,\n",
       "         11.8167, 11.8167, 11.8167, 11.8167, 11.8167, 11.9000, 11.9000, 11.9000,\n",
       "         11.9000, 11.9000, 11.9833, 11.9833, 11.9833, 11.9833, 11.9833, 12.0667,\n",
       "         12.0667, 12.0667, 12.0667, 12.0667, 12.1500, 12.1500, 12.1500, 12.1500,\n",
       "         12.2333, 12.2333, 12.2333, 12.2333, 12.2333, 12.2333, 12.2667, 12.2667,\n",
       "         12.3167, 12.3167, 12.3167, 12.3167, 12.3167, 12.3833, 12.4833, 12.4833,\n",
       "         12.4833, 12.4833, 12.4833, 12.7333, 12.7333, 12.7333, 12.7333, 12.7333,\n",
       "         12.9833, 12.9833, 12.9833, 12.9833, 12.9833, 12.9833, 12.9833, 12.9833,\n",
       "         13.2333, 13.2333, 13.2333, 13.2333, 13.2333, 13.2333, 13.2333, 13.2333,\n",
       "         13.2333, 13.4833, 13.4833, 13.4833, 13.4833, 13.4833, 13.5667, 13.5667,\n",
       "         13.5667, 13.5667, 13.5667, 13.6500, 13.6500, 13.6500, 13.6500, 13.6500,\n",
       "         13.7333, 13.7333, 13.7333, 13.7333, 13.7333, 13.8167, 13.8167, 13.8167,\n",
       "         13.8167, 13.8167, 13.9000, 13.9000, 13.9000, 13.9000, 13.9000, 13.9833,\n",
       "         13.9833, 13.9833, 13.9833, 13.9833, 14.0667, 14.0667, 14.0667, 14.0667,\n",
       "         14.0667, 14.2333, 14.2333, 14.2333, 14.2333, 14.2333, 14.2333, 14.2333,\n",
       "         14.2333, 14.2333, 14.2333, 14.7333, 14.7333, 14.7333, 14.7333, 14.7333,\n",
       "         14.7333, 15.2333, 15.2333, 15.2333, 15.2333, 15.2333, 15.2333, 15.2500,\n",
       "         15.2833, 15.2833, 15.7333, 15.7333, 15.7333, 15.7333, 15.7333, 15.7333,\n",
       "         16.2333, 16.2333, 16.2333, 16.2333, 16.2333, 16.2333, 16.2333, 16.5333,\n",
       "         16.5333, 17.2333, 17.2333, 17.2333, 17.2333, 17.2333, 17.2333, 17.2333,\n",
       "         17.2333, 17.2333, 17.2333, 18.2333, 18.2333, 18.2333, 18.2333, 18.2333,\n",
       "         18.2333, 18.2333, 18.2333, 18.2333, 18.2333, 19.2333, 19.2333, 19.2333,\n",
       "         19.2333, 19.2333, 19.2333, 19.5167, 19.5167, 19.5333, 19.7333, 19.7333,\n",
       "         19.7333, 19.7333, 19.7333, 19.7333, 20.2333, 20.2333, 20.2333, 20.2333,\n",
       "         20.2333, 20.2333, 20.2333, 20.2333, 20.2333, 20.2333, 20.2333, 20.2333,\n",
       "         20.7333, 20.7333, 20.7333, 20.7333, 20.7333, 20.7333, 20.7333, 20.8833,\n",
       "         20.8833, 20.8833, 21.2333, 21.2333, 21.2333, 21.2333, 21.2333, 21.2333,\n",
       "         21.2333, 21.2333, 21.2333, 21.2333, 21.2333, 21.2333, 22.2333, 22.2333,\n",
       "         22.2333, 22.2333, 22.2333, 22.2333, 22.2333, 23.2167, 23.2333, 23.2333,\n",
       "         23.2333, 23.2333, 23.2333, 23.2333, 23.6500, 23.6500, 23.6500, 23.7333,\n",
       "         23.7333, 23.7333, 23.7333, 23.7333, 23.7333, 24.2333, 24.2333, 24.2333,\n",
       "         24.2333, 24.2333, 24.2333, 24.2333, 24.2333, 24.2333, 24.2333, 24.2333,\n",
       "         24.2833, 24.4833, 24.4833, 24.4833, 24.4833, 24.4833, 24.4833, 24.4833,\n",
       "         24.7333, 24.7333, 24.7333, 24.7333, 24.7333, 24.7333, 24.7333, 24.9833,\n",
       "         24.9833, 24.9833, 24.9833, 24.9833, 24.9833, 24.9833, 25.2333, 25.2333,\n",
       "         25.2333, 25.2333, 25.2333, 25.2333, 25.2333, 25.2333, 25.2333, 25.2333,\n",
       "         25.2333, 25.6500, 25.6500, 25.9833, 26.2333, 26.2333, 26.2333, 26.2333,\n",
       "         26.2333, 26.2333, 26.2333, 27.2333, 27.2333, 27.2333, 27.2333, 27.2333,\n",
       "         27.2333, 27.2333, 27.7333, 27.7333, 27.7333, 27.7333, 27.7333, 27.7333,\n",
       "         27.7333, 28.2333, 28.2333, 28.2333],\n",
       "        [ 0.6672,  0.6672,  0.6672,  0.6672,  0.6672,  0.6672,  0.6672,  0.6672,\n",
       "          0.6672,  0.6672,  0.6672,  0.6672,  0.9172,  1.1672,  1.1672,  1.1672,\n",
       "          1.1672,  1.1672,  1.1672,  2.1672,  2.1672,  2.1672,  2.1672,  2.1672,\n",
       "          2.1672,  2.1672,  3.1672,  3.1672,  3.1672,  3.1672,  3.1672,  3.1672,\n",
       "          3.1672,  3.5839,  3.5839,  3.5839,  3.5839,  3.5839,  3.5839,  4.1672,\n",
       "          4.1672,  4.1672,  4.1672,  4.1672,  4.1672,  4.1672,  5.1672,  5.1672,\n",
       "          5.1672,  5.1672,  5.1672,  5.1672,  5.1672,  5.1672,  5.1672,  5.1672,\n",
       "          5.1672,  6.1672,  6.1672,  6.1672,  6.1672,  6.1672,  6.1672,  7.1672,\n",
       "          7.1672,  7.1672,  7.1672,  7.1672,  7.1672,  7.1672,  7.3172,  8.1672,\n",
       "          8.1672,  8.1672,  8.1672,  8.1672,  8.1672,  9.1672,  9.1672,  9.1672,\n",
       "          9.1672,  9.1672,  9.1672,  9.1672,  9.1672,  9.1672,  9.1672,  9.1672,\n",
       "          9.1672, 10.1672, 10.1672, 10.1672, 10.1672, 10.1672, 10.1672, 11.1672,\n",
       "         11.1672, 11.1672, 11.1672, 11.1672, 11.1672, 12.1672, 12.1672, 12.1672,\n",
       "         12.1672, 12.1672, 12.1672, 13.1672, 13.1672, 13.1672, 13.1672, 13.1672,\n",
       "         13.1672, 13.1672, 13.1672, 13.1672, 13.1672, 13.1672, 21.1672, 21.1672,\n",
       "         21.1672, 21.1672, 21.1672, 21.1672, 21.1672, 21.1672, 21.1672, 21.1672,\n",
       "         21.1672, 22.1672, 22.1672, 22.1672, 22.1672, 22.1672, 22.1672, 22.6672,\n",
       "         23.0006, 25.1672, 25.1672, 25.1672, 25.1672, 25.1672, 25.1672, 25.1672,\n",
       "         25.1672, 25.1672, 25.1672, 25.1672, 26.1672, 26.1672, 26.1672, 26.1672,\n",
       "         26.1672, 26.1672, 27.1672, 27.1672, 27.1672, 27.1672, 27.1672, 27.1672,\n",
       "         27.1672, 28.1672, 28.1672, 28.1672, 28.1672, 28.1672, 28.1672, 29.1672,\n",
       "         29.1672, 29.1672, 29.1672, 29.1672, 29.1672, 29.1672, 29.1672, 29.1672,\n",
       "         29.1672, 29.1672, 31.1672, 31.1672, 31.1672, 31.1672, 31.1672, 31.1672,\n",
       "         33.1672, 33.1672, 33.1672, 33.1672, 33.1672, 33.1672, 33.1672, 33.7006,\n",
       "         34.1672, 34.1672, 35.1672, 35.1672, 35.1672, 35.1672, 35.1672, 35.1672,\n",
       "         35.1672, 35.1672, 35.1672, 35.1672, 37.1672, 37.1672, 37.1672, 37.1672,\n",
       "         37.1672, 37.1672, 37.1672, 37.1672, 37.1672, 37.1672, 37.1672, 39.1672,\n",
       "         39.1672, 39.1672, 39.1672, 39.1672, 39.1672, 41.1672, 41.1672, 41.1672,\n",
       "         41.1672, 41.1672, 41.1672, 41.1672, 41.1672, 41.1672, 41.1672, 41.1672,\n",
       "         41.1672, 43.1672, 43.1672, 43.1672, 43.1672, 43.1672, 43.1672, 45.1672,\n",
       "         45.1672, 45.1672, 45.1672, 45.1672, 45.1672, 45.1672, 45.1672, 45.1672,\n",
       "         45.1672, 46.1672, 47.1672, 47.1672, 47.1672, 47.1672, 47.1672, 47.1672,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0000,  0.0000,  0.0000],\n",
       "        [22.6000, 34.8167, 46.1333]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_text_times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 3])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_text_times.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 3, 1024])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_text_tokens.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 3])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_text_feature_varis.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 500])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_varis.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1., 0.])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 3, 1024])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_text_tokens.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 3, 1024])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_text_attention_mask.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[    0,     0,     0,  ...,     0,     0,     0],\n",
       "         [    0,     0,     0,  ...,     0,     0,     0],\n",
       "         [    0,     0,     0,  ...,     0,     0,     0]],\n",
       "\n",
       "        [[    0,   282, 35857,  ...,     1,     1,     1],\n",
       "         [    0,    90,    73,  ...,     1,     1,     1],\n",
       "         [    0,  7048,   575,  ...,     1,     1,     1]]])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_text_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Numerical Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, val, test, V, D = load_mortality_dataset(\n",
    "    data_dir='./mortality_datasets', \n",
    "    with_text=False, \n",
    "    period_length=48, \n",
    "    debug=False\n",
    ")\n",
    "\n",
    "train_dataloader = DataLoader(train, batch_size=4, collate_fn=normalise_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "custom_STraTS(\n",
      "  (values_stack): CVE(\n",
      "    (stack): Sequential(\n",
      "      (0): Linear(in_features=1, out_features=8, bias=True)\n",
      "      (1): Tanh()\n",
      "      (2): Linear(in_features=8, out_features=64, bias=False)\n",
      "    )\n",
      "  )\n",
      "  (times_stack): CVE(\n",
      "    (stack): Sequential(\n",
      "      (0): Linear(in_features=1, out_features=8, bias=True)\n",
      "      (1): Tanh()\n",
      "      (2): Linear(in_features=8, out_features=64, bias=False)\n",
      "    )\n",
      "  )\n",
      "  (varis_stack): Embedding(130, 64)\n",
      "  (mTAND): MultiTimeAttention(\n",
      "    (linears): ModuleList(\n",
      "      (0-1): 2 x Linear(in_features=64, out_features=64, bias=True)\n",
      "      (2): Linear(in_features=256, out_features=64, bias=True)\n",
      "    )\n",
      "  )\n",
      "  (CTE): STraTS_Transformer(\n",
      "    (dropout_layer): Dropout(p=0.1, inplace=False)\n",
      "    (identity): Identity()\n",
      "  )\n",
      "  (atten_stack): Attention(\n",
      "    (stack): Sequential(\n",
      "      (0): Linear(in_features=64, out_features=128, bias=True)\n",
      "      (1): Tanh()\n",
      "      (2): Linear(in_features=128, out_features=1, bias=False)\n",
      "    )\n",
      "    (softmax): Softmax(dim=-2)\n",
      "  )\n",
      "  (demo_stack): Sequential(\n",
      "    (0): Linear(in_features=2, out_features=128, bias=True)\n",
      "    (1): Tanh()\n",
      "    (2): Linear(in_features=128, out_features=64, bias=True)\n",
      "    (3): Tanh()\n",
      "  )\n",
      "  (output_stack): Sequential(\n",
      "    (0): Linear(in_features=128, out_features=1, bias=True)\n",
      "    (1): Sigmoid()\n",
      "    (2): Flatten(start_dim=0, end_dim=-1)\n",
      "  )\n",
      ")\n",
      "Total number of parameters: 117289, Total trainable parameters: 117289\n"
     ]
    }
   ],
   "source": [
    "from orig_model import STraTS\n",
    "from new_model import custom_STraTS\n",
    "\n",
    "# text_model, config, tokenizer = load_Bert(\n",
    "#     text_encoder_model = 'bioLongformer'\n",
    "# )\n",
    "\n",
    "# model = STraTS(\n",
    "#     D=D, # No. of static variables\n",
    "#     V=V+1, # No. of variables / features\n",
    "#     d=64, # Input size of attention layer\n",
    "#     N=2, # No. of Encoder blocks\n",
    "#     he=4, # No. of heads in multi headed encoder blocks\n",
    "#     dropout=0, \n",
    "#     with_text=False,\n",
    "#     forecast=False, \n",
    "#     return_embeddings=False\n",
    "# )\n",
    "\n",
    "model = custom_STraTS(\n",
    "    D=D,\n",
    "    V=V,\n",
    "    d=64,\n",
    "    N=2,\n",
    "    he=4,\n",
    "    dropout=0.1,\n",
    "    time_2_vec=False,\n",
    ")\n",
    "\n",
    "print(model)\n",
    "\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "total_trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad==True)\n",
    "print(f'Total number of parameters: {total_params}, Total trainable parameters: {total_trainable_params}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from accelerate import Accelerator\n",
    "\n",
    "# accelerator = Accelerator()\n",
    "# accelerator.device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.0005)\n",
    "\n",
    "from utils import mortality_loss\n",
    "loss_fn = mortality_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model, optimizer, train_dataloader = accelerator.prepare(model, optimizer, train_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n",
      "tensor([[ 1.9333, 13.9667,  8.8333,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [16.9000, 10.9000, 11.9000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 6.9000,  7.9000,  6.4000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [17.7833, 20.7833, 19.7833,  ...,  0.0000,  0.0000,  0.0000]])\n",
      "ts_varis_emb: torch.Size([4, 880, 64])\n",
      "ts_values_emb: torch.Size([4, 880, 64])\n",
      "ts_times_emb: torch.Size([4, 880, 64])\n",
      "mask: torch.Size([4, 880])\n",
      " MultiTime Attention query: torch.Size([4, 4, 880, 16])\n",
      " MultiTime Attention key: torch.Size([4, 4, 880, 16])\n",
      " MultiTime Attention value: torch.Size([4, 1, 880, 64])\n",
      " MultiTime Attention scores: torch.Size([4, 4, 880, 880])\n",
      " MultiTime Attention scores: torch.Size([4, 4, 880, 880, 1])\n",
      " MultiTime Attention mask: torch.Size([4, 1, 880, 1])\n",
      " MultiTime Attention p_attn: torch.Size([4, 4, 880, 880, 1])\n",
      " MultiTime Attention value.unsqueeze(-3): torch.Size([4, 1, 1, 880, 64])\n",
      " MultiTime Attention output: torch.Size([4, 4, 880, 64])\n",
      " MultiTime Attention output: torch.Size([4, 880, 256])\n",
      "time_atten: torch.Size([4, 880, 64])\n",
      "CTE_emb: torch.Size([4, 880, 64])\n",
      "attn_weights: torch.Size([4, 880, 1])\n",
      "fused_emb: torch.Size([4, 64])\n",
      "conc: torch.Size([4, 128])\n"
     ]
    }
   ],
   "source": [
    "batch = next(iter(train_dataloader))\n",
    "\n",
    "# X_demos, X_times, X_values, X_varis, Y, X_text_tokens, X_text_attention_mask, X_text_times, X_text_time_mask, X_text_feature_varis = iter(train_dataloader).next()\n",
    "# Y_pred = model(X_demos, X_times, X_values, X_varis, X_text_tokens, X_text_attention_mask, X_text_times, X_text_feature_varis)\n",
    "\n",
    "X_demos, X_times, X_values, X_varis, Y = batch\n",
    "Y_pred = model(X_demos, X_times, X_values, X_varis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.7143, grad_fn=<BinaryCrossEntropyBackward0>)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss = loss_fn(Y, Y_pred)\n",
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer.step()\n",
    "optimizer.zero_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.5504, 0.5166, 0.5275, 0.5432], grad_fn=<ViewBackward0>)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1., 0., 0., 0.])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fyp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
